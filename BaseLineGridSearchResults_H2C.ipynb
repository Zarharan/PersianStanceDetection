{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BaseLineGridSearchResults.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oBz3rNazY1nz",
        "afMJ4aqRBvQ0",
        "6Mb30c58Byq0",
        "dj_wNk4ibwqm",
        "XyjAncnj9rUd",
        "9uQkO49tRZRO",
        "x6TxEMnqMyok",
        "rk88fMmPM4pG",
        "Yk3iW_TZNI4q",
        "dr5NGUtabYhK"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBz3rNazY1nz"
      },
      "source": [
        "# Install and Import Required Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afMJ4aqRBvQ0"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SMG5tHCzkOA",
        "outputId": "6bd25074-ba08-443f-8661-cce485022032",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install hazm\n",
        "!pip install stanfordnlp\n",
        "!pip install -U nltk\n",
        "!pip install pandas\n",
        "!pip install sklearn\n",
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 8.7MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.12.0)\n",
            "Building wheels for collected packages: libwapiti, nltk\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp36-cp36m-linux_x86_64.whl size=154256 sha256=0ccfa12f99ccb005d08483680fec13a00b4b94c0e243afd2c6a64ddfb14d0071\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394470 sha256=49f93807551cc921028ac5729ac72876b287761bcd32c30df43d5c4c6d01b141\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built libwapiti nltk\n",
            "Installing collected packages: libwapiti, nltk, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Collecting stanfordnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/bf/5d2898febb6e993fcccd90484cba3c46353658511a41430012e901824e94/stanfordnlp-0.2.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.10.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.5.0+cu101)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (47.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->stanfordnlp) (0.16.0)\n",
            "Installing collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n",
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.41.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434675 sha256=b94e38903f2afcfa970b621956887b881312f8cd35fe1eb8f9d4d69ae04a33a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "\u001b[31mERROR: hazm 0.7.0 has requirement nltk==3.3, but you'll have nltk 3.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.3\n",
            "    Uninstalling nltk-3.3:\n",
            "      Successfully uninstalled nltk-3.3\n",
            "Successfully installed nltk-3.5\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mb30c58Byq0"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLQ0-YGbBoFm",
        "outputId": "18cad59d-c535-44bf-ded7-c82ee3f349a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import shuffle\n",
        "import stanfordnlp\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj_wNk4ibwqm"
      },
      "source": [
        "# Import my class from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDqlb6kRb6aO",
        "outputId": "f5801a49-897e-43d5-dbae-3ffe06041c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "FeatureExtractor = drive.CreateFile({'id':'1IwEfIW-lYHvGSSr6TwMsGX3xX1Z829cP'})\n",
        "FeatureExtractor.GetContentFile('psfeatureextractor.py')\n",
        "from psfeatureextractor import PSFeatureExtractor as FeatureExtractor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4W2-_aXCAXX"
      },
      "source": [
        "# Common Functions and Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxF5KWtM_3-d"
      },
      "source": [
        "stanford_models_path  = '/content/drive/My Drive/Stance Detection Paper/persian_stance_baseline_data/' \n",
        "dataset_path = '/content/drive/My Drive/Stance Detection Paper/HeadlineToClaim.csv'\n",
        "stopWord_path = '/content/drive/My Drive/ImportantNLPFiles/StopWords_fa.txt'\n",
        "polarity_dataset_path = '/content/drive/My Drive/Stance Detection Paper/PolarityDataset.xlsx'\n",
        "save_load_path = \"/content/drive/My Drive/Stance Detection Paper/persian_stance_baseline_data/vectors\"\n",
        "w2v_model_path = \"/content/drive/My Drive/Stance Detection Paper/persian_stance_baseline_data/vectors/w2v_persian.pkl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh4V5QklSxlI"
      },
      "source": [
        "def k_fold_train_test(X, Y, k_fold, model, scoring = 'accuracy' , additional_description = ''):\n",
        "  model_name = model.__class__.__name__\n",
        "  accuracies = cross_val_score(model, X, Y, scoring = scoring, cv= k_fold)\n",
        "  result = []\n",
        "  df_result = pd.DataFrame(index=range(k_fold))\n",
        "  for fold_index, accuracy in enumerate(accuracies):\n",
        "    result.append((model_name, fold_index, accuracy))\n",
        "  \n",
        "  df_result = pd.DataFrame(result, columns=['model_name', 'fold_index', scoring])\n",
        "\n",
        "  sns.boxplot(x = 'model_name', y = scoring, data = df_result)\n",
        "  sns.stripplot(x='model_name', y=scoring, data = df_result, \n",
        "                size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
        "  plt.show()\n",
        "\n",
        "  print('Mean '+scoring + ' of ' + model_name + ' in ' + str(k_fold) + ' fold is: ', np.average(accuracies, axis=0))\n",
        "  if len(additional_description)>0:\n",
        "    print(additional_description)\n",
        "  return df_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlVZ2DJD9gJO"
      },
      "source": [
        "def common_train_test(model, X, Y, test_size= 0.2, additional_description = ''):\n",
        "  \n",
        "  model_name = model.__class__.__name__\n",
        "  # Todo : اول اینکه هر جا سید داریم ثابتش کنیم، و اینکه ترین و تست ست سیو شود و در حالت سیو شده فقط لود شود  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle= True , test_size = test_size, random_state = 0)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  conf_mat = confusion_matrix(y_test, y_pred)\n",
        "  fig, ax = plt.subplots(figsize=(4,4))\n",
        "  labels_name = np.unique(Y)\n",
        "  sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels = labels_name, yticklabels = labels_name)\n",
        "  plt.ylabel('Actual')\n",
        "  plt.xlabel('Predicted')\n",
        "  print(model_name)\n",
        "  plt.show()\n",
        "\n",
        "  print(metrics.classification_report(y_test, y_pred, labels_name))\n",
        "  print('accuracy : ', accuracy_score(y_test, y_pred))\n",
        "  print('weighted f1 score : ', f1_score(y_test, y_pred, average='weighted'))\n",
        "  if len(additional_description)>0:\n",
        "    print(additional_description)\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6_1UxzMDgrQ"
      },
      "source": [
        "def grid_search_train_test(all_grid_params, X, Y, test_size= 0.2, additional_description = '', randomize = False\n",
        "                           , n_iter_search = 100):\n",
        "  \n",
        "  model_name = all_grid_params[\"estimator\"].__class__.__name__\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle= True , test_size = test_size, random_state = 0)\n",
        "\n",
        "  grid_result =  None\n",
        "  if randomize:\n",
        "    print(\"Randomized grid search started.\")\n",
        "    clf = RandomizedSearchCV(estimator = all_grid_params[\"estimator\"], param_distributions = all_grid_params[\"param_grid\"]\n",
        "                      , scoring = all_grid_params[\"scoring\"], cv = all_grid_params[\"cv\"]\n",
        "                      , verbose = all_grid_params[\"verbose\"], n_jobs = all_grid_params[\"n_jobs\"], n_iter=n_iter_search )\n",
        "    grid_result = clf.fit(X_train, y_train)\n",
        "\n",
        "    print('The best parameters for ' + model_name + ' is: ')\n",
        "    print(grid_result.best_estimator_)\n",
        "  else:\n",
        "    print(\"Full grid search started.\")\n",
        "    clf = GridSearchCV(estimator = all_grid_params[\"estimator\"], param_grid = all_grid_params[\"param_grid\"]\n",
        "                      , scoring = all_grid_params[\"scoring\"], cv = all_grid_params[\"cv\"]\n",
        "                      , verbose = all_grid_params[\"verbose\"], n_jobs = all_grid_params[\"n_jobs\"])\n",
        "    grid_result = clf.fit(X_train, y_train)\n",
        "\n",
        "    print('The best parameters for ' + model_name + ' is: ')\n",
        "    print(grid_result.best_estimator_)\n",
        "\n",
        "  if len(additional_description)>0:\n",
        "    print(additional_description)\n",
        "  return ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-qxFD3tQYY-"
      },
      "source": [
        "def get_best_models(all_features, all_labels, features_name, use_svm= True, use_random_forest= True\n",
        "                    , use_linear_svc= True, use_logistic_regression= True, use_GussianNB= True, k_fold = 10\n",
        "                    , randomize = False, n_iter_search = 100):\n",
        "  # estimator: estimator object you created\n",
        "  # params_grid: the dictionary object that holds the hyperparameters you want to try\n",
        "  # scoring: evaluation metric that you want to use, you can simply pass a valid string/ object of evaluation metric\n",
        "  # cv: number of cross-validation you have to try for each selected set of hyperparameters\n",
        "  # verbose: you can set it to 1 to get the detailed print out while you fit the data to GridSearchCV\n",
        "  # n_jobs: number of processes you wish to run in parallel for this task if it -1 it will use all available processors.\n",
        "  additional_description = 'With the features : '+ features_name\n",
        "  if use_svm:\n",
        "    all_grid_params = {\n",
        "      \"estimator\": SVC(),\n",
        "      \"param_grid\": {'kernel':('poly', 'rbf', 'sigmoid'), 'C':[1, 10], \"degree\":[3,5]\n",
        "                    , \"class_weight\":('dict','balanced','None'), \"decision_function_shape\": ('ovo', 'ovr') },\n",
        "      \"scoring\": \"f1_weighted\",\n",
        "      \"cv\" : k_fold,\n",
        "      \"verbose\": 1,\n",
        "      \"n_jobs\" : -1\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      result = grid_search_train_test(all_grid_params = all_grid_params, X= all_features, Y= all_labels, test_size = 0.2\n",
        "                              , additional_description = additional_description, randomize = randomize, n_iter_search=n_iter_search)\n",
        "    except:\n",
        "      print('An error accured when using grid_search_train_test for SVC model.')\n",
        "  \n",
        "  if use_random_forest:\n",
        "    all_grid_params = {\n",
        "      \"estimator\": RandomForestClassifier(),\n",
        "      \"param_grid\": {'criterion':('gini','entropy'), 'n_estimators':[50, 75, 100, 125, 150, 175, 200]\n",
        "                    , \"class_weight\":('dict','balanced','balanced_subsample','None')},\n",
        "      \"scoring\": \"f1_weighted\",\n",
        "      \"cv\" : k_fold,\n",
        "      \"verbose\": 1,\n",
        "      \"n_jobs\" : -1\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      result = grid_search_train_test(all_grid_params = all_grid_params, X= all_features, Y= all_labels, test_size = 0.2\n",
        "                              , additional_description = additional_description, randomize = randomize, n_iter_search=n_iter_search)\n",
        "    except:\n",
        "      print('An error accured when using grid_search_train_test for RandomForestClassifier model.')\n",
        "\n",
        "  if use_linear_svc:\n",
        "    all_grid_params = {\n",
        "      \"estimator\": LinearSVC(),\n",
        "      \"param_grid\": {'penalty':('l1','l2'), 'C':[0.5, 1, 1.5, 2, 2.5], \"multi_class\":(\"ovr\",\"crammer_singer\")\n",
        "                    , \"loss\":('hinge','squared_hinge'), \"class_weight\":(\"dict\", \"balanced\", \"None\")\n",
        "                    ,\"max_iter\":[1000, 1200]},\n",
        "      \"scoring\": \"f1_weighted\",\n",
        "      \"cv\" : k_fold,\n",
        "      \"verbose\": 1,\n",
        "      \"n_jobs\" : -1\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      result = grid_search_train_test(all_grid_params = all_grid_params, X= all_features, Y= all_labels, test_size = 0.2\n",
        "                              , additional_description = additional_description, randomize = randomize, n_iter_search=n_iter_search)\n",
        "    except:\n",
        "      print('An error accured when using grid_search_train_test for LinearSVC model.')\n",
        "\n",
        "  if use_logistic_regression:\n",
        "    all_grid_params = {\n",
        "      \"estimator\": LogisticRegression(),\n",
        "      \"param_grid\": {'penalty':('l1','l2'), 'C':[0.5, 0.75 ,1]\n",
        "                    , \"solver\":('newton-cg', 'sag', 'saga')\n",
        "                    , \"class_weight\":[\"balanced\"]\n",
        "                    ,\"max_iter\":[1000, 1200], \"multi_class\":('ovr', 'multinomial')\n",
        "                    },\n",
        "      \"scoring\": \"f1_weighted\",\n",
        "      \"cv\" : k_fold,\n",
        "      \"verbose\": 1,\n",
        "      \"n_jobs\" : -1\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      result = grid_search_train_test(all_grid_params = all_grid_params, X= all_features, Y= all_labels, test_size = 0.2\n",
        "                              , additional_description = additional_description, randomize = randomize, n_iter_search=n_iter_search)\n",
        "    except Exception as error:\n",
        "      print('An error accured when using grid_search_train_test for LogisticRegression model.')\n",
        "      print(error.args)\n",
        "\n",
        "  if use_GussianNB:\n",
        "    all_grid_params = {\n",
        "      \"estimator\": GaussianNB(),\n",
        "      \"param_grid\": {},\n",
        "      \"scoring\": \"f1_weighted\",\n",
        "      \"cv\" : k_fold,\n",
        "      \"verbose\": 1,\n",
        "      \"n_jobs\" : -1\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      result = grid_search_train_test(all_grid_params = all_grid_params, X= all_features, Y= all_labels, test_size = 0.2\n",
        "                              , additional_description = additional_description, randomize = randomize, n_iter_search=n_iter_search)\n",
        "    except:\n",
        "      print('An error accured when using grid_search_train_test for GaussianNB model.')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPTiAwfAAPuk"
      },
      "source": [
        "## Create Feature Extractor Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAGzuYtgALiv",
        "outputId": "9936b5b0-ea76-4d3f-fb42-46a57a586f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "psf_extractor = FeatureExtractor(dataset_path = dataset_path, stopWord_path = stopWord_path\n",
        "                                    , polarity_dataset_path = polarity_dataset_path,\n",
        "                                  stanford_models_path = stanford_models_path\n",
        "                                  ,use_google_drive = True, important_words = ['؟',\n",
        "             'تکذیب',\n",
        "             'تکذیب شد',\n",
        "             ':',\n",
        "             ])\n",
        "\n",
        "tokens_claims , tokens_headlines = psf_extractor.nltk_tokenize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/ImportantNLPFiles/StopWords_fa.txt\n",
            "(2029,) (2029,) (2029,) (2029,) (2029,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfOh7sH9A701",
        "outputId": "249c4581-ad65-4054-91c7-8db9a98fc210",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels = np.reshape(psf_extractor.labels,(len(psf_extractor.labels),1))\n",
        "labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2029, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IfJJJ6NK_Hy",
        "outputId": "0474f978-1625-44fb-8554-33b2a13420d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "metrics.SCORERS.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyjAncnj9rUd"
      },
      "source": [
        "# TF-IDF With Other Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uQkO49tRZRO"
      },
      "source": [
        "## Extract Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnDF49XjRYOI",
        "outputId": "35181fb6-fa75-4308-c827-3bcf63a74386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "features, features_name = psf_extractor.generate_Features(w2v_model_path = w2v_model_path,save_path = save_load_path\n",
        "                                                          , save_feature= True\n",
        "                                                          , load_path= save_load_path\n",
        "                                                          , load_if_exist = True, bow = False, w2v = False, polarity= False)\n",
        "\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features loaded successfully.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2029, 728)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7RfN_fNRjHW"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g__yEpzIROdX",
        "outputId": "08099a42-ba7e-42aa-d20d-ae9d1141b0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "get_best_models(features, labels, features_name,use_logistic_regression = False, use_GussianNB = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters for SVC is: \n",
            "SVC(C=10, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
            "    decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n",
            "With the features : tfidf_similarity_important_words_more_than2_parts_root_distance_\n",
            "The best parameters for RandomForestClassifier is: \n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
            "                       class_weight='balanced_subsample', criterion='entropy',\n",
            "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "                       max_samples=None, min_impurity_decrease=0.0,\n",
            "                       min_impurity_split=None, min_samples_leaf=1,\n",
            "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                       n_estimators=75, n_jobs=None, oob_score=False,\n",
            "                       random_state=None, verbose=0, warm_start=False)\n",
            "With the features : tfidf_similarity_important_words_more_than2_parts_root_distance_\n",
            "The best parameters for LinearSVC is: \n",
            "LinearSVC(C=0.5, class_weight='balanced', dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
            "          penalty='l2', random_state=None, tol=0.0001, verbose=0)\n",
            "With the features : tfidf_similarity_important_words_more_than2_parts_root_distance_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnPx86aFI3CQ",
        "outputId": "ed732dda-bd9c-4286-c7f8-0d6c73a25a2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "get_best_models(features, labels, features_name, use_svm= False, use_random_forest= False\n",
        "                    , use_linear_svc= False,use_GussianNB = False, k_fold=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:  3.7min\n",
            "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed: 18.0min\n",
            "[Parallel(n_jobs=-1)]: Done 450 tasks      | elapsed: 43.0min\n",
            "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 46.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for LogisticRegression is: \n",
            "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='ovr', n_jobs=None, penalty='l1',\n",
            "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "With the features : tfidf_similarity_important_words_more_than2_parts_root_distance_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZLreFwYHDrr",
        "outputId": "7a6a0bed-08d5-4faf-d646-abe0e6eb1117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "get_best_models(features, labels, features_name, use_svm= False, use_random_forest= False\n",
        "                    , use_linear_svc= False,use_logistic_regression = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters for GaussianNB is: \n",
            "GaussianNB(priors=None, var_smoothing=1e-09)\n",
            "With the features : tfidf_similarity_important_words_more_than2_parts_root_distance_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5xu5C8l92m1"
      },
      "source": [
        "# BOW With Other Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHJRoe2STdjs"
      },
      "source": [
        "## Extract Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UDNrXtuBDrb",
        "outputId": "db42dad3-50bf-4529-f08f-5c42e599c81b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "features, features_name = psf_extractor.generate_Features(w2v_model_path = w2v_model_path,save_path = save_load_path\n",
        "                                                          , save_feature= True\n",
        "                                                          , load_path= save_load_path\n",
        "                                                          , load_if_exist = True, tfidf = False, w2v = False, polarity= False)\n",
        "\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features loaded successfully.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2029, 20360)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT93z2fPThoG"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXP5GwLLTh-U",
        "outputId": "e3568ff4-bfb6-4a60-cf7a-03746f79ec3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "get_best_models(features, labels, features_name,use_logistic_regression = False, use_GussianNB = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    4.5s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 67.0min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 105.2min\n",
            "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 207.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for SVC is: \n",
            "SVC(C=10, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
            "    decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n",
            "With the features : similarity_important_words_more_than2_parts_root_distance_bow_\n",
            "Fitting 10 folds for each of 56 candidates, totalling 560 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    6.4s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  8.1min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 44.7min\n",
            "[Parallel(n_jobs=-1)]: Done 560 out of 560 | elapsed: 44.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for RandomForestClassifier is: \n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
            "                       class_weight='balanced_subsample', criterion='gini',\n",
            "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "                       max_samples=None, min_impurity_decrease=0.0,\n",
            "                       min_impurity_split=None, min_samples_leaf=1,\n",
            "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                       n_estimators=75, n_jobs=None, oob_score=False,\n",
            "                       random_state=None, verbose=0, warm_start=False)\n",
            "With the features : similarity_important_words_more_than2_parts_root_distance_bow_\n",
            "Fitting 10 folds for each of 240 candidates, totalling 2400 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    3.5s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   52.0s\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  7.3min\n",
            "[Parallel(n_jobs=-1)]: Done 1246 tasks      | elapsed: 10.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1796 tasks      | elapsed: 15.6min\n",
            "[Parallel(n_jobs=-1)]: Done 2400 out of 2400 | elapsed: 19.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for LinearSVC is: \n",
            "LinearSVC(C=1, class_weight='balanced', dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1200,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
            "          verbose=0)\n",
            "With the features : similarity_important_words_more_than2_parts_root_distance_bow_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwGTazeXVJ2W",
        "outputId": "1c190b66-532e-4cd7-d49f-57b54dbe8cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "get_best_models(features, labels, features_name, use_svm= False, use_random_forest= False\n",
        "                    , use_linear_svc= False,use_GussianNB = False, k_fold=5,randomize = True, n_iter_search = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Randomized grid search started.\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 382.4min\n",
            "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 398.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for LogisticRegression is: \n",
            "LogisticRegression(C=0.75, class_weight='balanced', dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=1000, multi_class='multinomial', n_jobs=None,\n",
            "                   penalty='l1', random_state=None, solver='saga', tol=0.0001,\n",
            "                   verbose=0, warm_start=False)\n",
            "With the features : similarity_important_words_is_question_more_than2_parts_root_distance_bow_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGpwPd1NVTeW",
        "outputId": "17b86337-68ff-4d46-8bc3-12c6dc041856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "\n",
        "\n",
        "get_best_models(features, labels, features_name, use_svm= False, use_random_forest= False\n",
        "                    , use_linear_svc= False,use_logistic_regression = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for GaussianNB is: \n",
            "GaussianNB(priors=None, var_smoothing=1e-09)\n",
            "With the features : similarity_important_words_more_than2_parts_root_distance_bow_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2deacxACVevC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6TxEMnqMyok"
      },
      "source": [
        "# W2V With Other Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk88fMmPM4pG"
      },
      "source": [
        "## Extract Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEMBr7EzM0ja",
        "outputId": "0a18c319-a53b-41fd-834d-ef39eeb146c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "features, features_name = psf_extractor.generate_Features(w2v_model_path = w2v_model_path,save_path = save_load_path\n",
        "                                                          , save_feature= True\n",
        "                                                          , load_path= save_load_path\n",
        "                                                          , load_if_exist = True, tfidf = False, bow = False, polarity= False)\n",
        "\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features vector file is not exist.\n",
            "Start to generate similarity feature\n",
            "End of similarity feature\n",
            "Start to generate important words feature\n",
            "End of important words feature\n",
            "\"is question\" feature was added.\n",
            "\"more than tow parts\" feature was added.\n",
            "Start to generate root distance feature\n",
            "Use device: gpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/content/drive/My Drive/Stance Detection Paper/persian_stance_baseline_data/fa_seraji_models/fa_seraji_tokenizer.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
            "---\n",
            "Loading: mwt\n",
            "With settings: \n",
            "{'model_path': '/content/drive/My Drive/Stance Detection Paper/persian_stance_baseline_data/fa_seraji_models/fa_seraji_mwt_expander.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/content/drive/My Drive/Stance Detection Paper/persian_stance_baseline_data/fa_seraji_models/fa_seraji_tagger.pt', 'pretrain_path': '/content/drive/My Drive/Stance Detection Paper/persian_stance_baseline_data/fa_seraji_models/fa_seraji.pretrain.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': '/content/drive/My Drive/Stance Detection Paper/persian_stance_baseline_data/fa_seraji_models/fa_seraji_lemmatizer.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "---\n",
            "Loading: depparse\n",
            "With settings: \n",
            "{'model_path': '/content/drive/My Drive/Stance Detection Paper/persian_stance_baseline_data/fa_seraji_models/fa_seraji_parser.pt', 'pretrain_path': '/content/drive/My Drive/Stance Detection Paper/persian_stance_baseline_data/fa_seraji_models/fa_seraji.pretrain.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "End of root distance feature\n",
            "Start to generate w2v feature\n",
            "data 0 of 2029\n",
            "data 1000 of 2029\n",
            "data 2000 of 2029\n",
            "End of w2v feature\n",
            "Features saved successfully.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2029, 310)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk3iW_TZNI4q"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTwE6OZyNAqF",
        "outputId": "d6a8398c-cf5f-4717-e915-2bdd20ccc5f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "get_best_models(features, labels, features_name,use_logistic_regression = False, use_GussianNB = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 243 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  5.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for SVC is: \n",
            "SVC(C=10, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
            "    decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n",
            "With the features : similarity_important_words_is_question_more_than2_parts_root_distance_w2v_\n",
            "Fitting 10 folds for each of 56 candidates, totalling 560 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:  6.8min\n",
            "[Parallel(n_jobs=-1)]: Done 424 tasks      | elapsed: 15.2min\n",
            "[Parallel(n_jobs=-1)]: Done 560 out of 560 | elapsed: 15.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for RandomForestClassifier is: \n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
            "                       class_weight='balanced_subsample', criterion='gini',\n",
            "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "                       max_samples=None, min_impurity_decrease=0.0,\n",
            "                       min_impurity_split=None, min_samples_leaf=1,\n",
            "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                       n_estimators=175, n_jobs=None, oob_score=False,\n",
            "                       random_state=None, verbose=0, warm_start=False)\n",
            "With the features : similarity_important_words_is_question_more_than2_parts_root_distance_w2v_\n",
            "Fitting 10 folds for each of 240 candidates, totalling 2400 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed: 29.6min\n",
            "[Parallel(n_jobs=-1)]: Done 851 tasks      | elapsed: 68.6min\n",
            "[Parallel(n_jobs=-1)]: Done 2230 tasks      | elapsed: 184.9min\n",
            "[Parallel(n_jobs=-1)]: Done 2400 out of 2400 | elapsed: 190.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for LinearSVC is: \n",
            "LinearSVC(C=1, class_weight='balanced', dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1200,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
            "          verbose=0)\n",
            "With the features : similarity_important_words_is_question_more_than2_parts_root_distance_w2v_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2hnDILbNC7D",
        "outputId": "0cb77763-a43d-4623-ec98-54ffddb4aad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "get_best_models(features, labels, features_name, use_svm= False, use_random_forest= False\n",
        "                    , use_linear_svc= False,use_GussianNB = True, k_fold=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:  7.6min\n",
            "[Parallel(n_jobs=-1)]: Done 462 tasks      | elapsed: 19.5min\n",
            "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 20.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for LogisticRegression is: \n",
            "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='sag', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "With the features : similarity_important_words_is_question_more_than2_parts_root_distance_w2v_\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "The best parameters for GaussianNB is: \n",
            "GaussianNB(priors=None, var_smoothing=1e-09)\n",
            "With the features : similarity_important_words_is_question_more_than2_parts_root_distance_w2v_\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg7uln7GOplS",
        "outputId": "0ad404be-bea7-4d2e-bc20-18f43fa1499a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_grid_params = {\n",
        "  \"estimator\": SVC(),\n",
        "  \"param_grid\": {'kernel':('poly', 'rbf', 'sigmoid'), 'C':[1, 10], \"degree\":[3,5]\n",
        "                , \"class_weight\":('dict','balanced','None'), \"decision_function_shape\": ('ovo', 'ovr') },\n",
        "  \"scoring\": \"f1_weighted\",\n",
        "  \"cv\" : 10,\n",
        "  \"verbose\": 1,\n",
        "  \"n_jobs\" : -1\n",
        "}\n",
        "# RandomizedSearchCV\n",
        "try:\n",
        "  result = grid_search_train_test(all_grid_params = all_grid_params, X= features, Y= labels, test_size = 0.2\n",
        "                          , additional_description = 'ssdd', search_type = \"random\")\n",
        "except:\n",
        "  print('An error accured when using grid_search_train_test for SVC model.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An error accured when using grid_search_train_test for SVC model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGdgUMTWS-Zh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr5NGUtabYhK"
      },
      "source": [
        "# W2V With TfIdf and Other Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stv8ypTJbkHN"
      },
      "source": [
        "## Extract Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBCDFGSJbjdu",
        "outputId": "3b83a3c0-12c6-4930-a0bd-cdb227c18bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "features, features_name = psf_extractor.generate_Features(w2v_model_path = w2v_model_path,save_path = save_load_path\n",
        "                                                          , save_feature= True\n",
        "                                                          , load_path= save_load_path\n",
        "                                                          , load_if_exist = True, bow = False, polarity= False)\n",
        "\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features loaded successfully.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2029, 1028)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGUY7MoDbv4L"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH932wGrbzBu",
        "outputId": "880eaa99-6fda-4f02-acc5-6e3a9d793286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "get_best_models(features, labels, features_name,use_logistic_regression = False, use_GussianNB = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 243 tasks      | elapsed:  4.2min\n",
            "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  8.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for SVC is: \n",
            "SVC(C=10, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
            "    decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n",
            "With the features : tfidf_similarity_important_words_is_question_more_than2_parts_root_distance_w2v_\n",
            "Fitting 10 folds for each of 56 candidates, totalling 560 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=-1)]: Done 390 tasks      | elapsed:  7.9min\n",
            "[Parallel(n_jobs=-1)]: Done 560 out of 560 | elapsed: 10.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for RandomForestClassifier is: \n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=125,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "With the features : tfidf_similarity_important_words_is_question_more_than2_parts_root_distance_w2v_\n",
            "Fitting 10 folds for each of 240 candidates, totalling 2400 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 270 tasks      | elapsed: 18.1min\n",
            "[Parallel(n_jobs=-1)]: Done 764 tasks      | elapsed: 74.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1687 tasks      | elapsed: 251.9min\n",
            "[Parallel(n_jobs=-1)]: Done 2400 out of 2400 | elapsed: 453.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for LinearSVC is: \n",
            "LinearSVC(C=0.5, class_weight='balanced', dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
            "          penalty='l2', random_state=None, tol=0.0001, verbose=0)\n",
            "With the features : tfidf_similarity_important_words_is_question_more_than2_parts_root_distance_w2v_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NUIHDRWbfha",
        "outputId": "591acc4a-ab1e-4076-8c02-6be555ff2334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "get_best_models(features, labels, features_name, use_svm= False, use_random_forest= False\n",
        "                    , use_linear_svc= False,use_GussianNB = True, k_fold=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:  9.0min\n",
            "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed: 28.2min\n",
            "[Parallel(n_jobs=-1)]: Done 462 tasks      | elapsed: 69.0min\n",
            "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 71.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for LogisticRegression is: \n",
            "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
            "                   multi_class='ovr', n_jobs=None, penalty='l1',\n",
            "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "With the features : tfidf_similarity_important_words_is_question_more_than2_parts_root_distance_w2v_\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The best parameters for GaussianNB is: \n",
            "GaussianNB(priors=None, var_smoothing=1e-09)\n",
            "With the features : tfidf_similarity_important_words_is_question_more_than2_parts_root_distance_w2v_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKAoCq20m5ES"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}