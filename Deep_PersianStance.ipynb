{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAgNOJBVVMQ_"
   },
   "source": [
    "# Persian Stance Classification - Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iMgbaBPU5wuC"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import datetime\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAvZfSsgZoRd"
   },
   "source": [
    "# Read Data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pvBMbjyZ7EN"
   },
   "source": [
    "**claim and body**\n",
    "\n",
    "```\n",
    "1997\n",
    "\n",
    "748\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5eNNmTGnZb4f",
    "outputId": "a11a702d-7bc4-4adf-86bf-81d9866907d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "GDRIVE_DIR = Path(\"/content/drive/My Drive\")\n",
    "\n",
    "dataset = pd.read_csv(GDRIVE_DIR / 'dataset' / 'ArticleToClaim.csv', index_col = 0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "25NC72fy3VV5"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "PWGYroc8aFa5",
    "outputId": "0e868c2e-8e7e-408c-e6c4-f0a0cbf3348b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim', 'body', 'question', 'part', 'headline', 'label'], dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLIiViZiabv8"
   },
   "outputs": [],
   "source": [
    "claim = dataset['claim']\n",
    "headline = dataset['headline']\n",
    "body = dataset['body']\n",
    "label = dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "IT_sz_7fbYq-",
    "outputId": "f8934d17-8a00-4ff7-d122-5ec0afd1363d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1997"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JV2OY21RatRc"
   },
   "outputs": [],
   "source": [
    "# print(claim[1], \"\\n\", body[1], \"\\n\", label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "P8S0PS0VzZ1X",
    "outputId": "f1b7b817-b283-4207-be14-1aca631845b7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAREAAAD7CAYAAABNPKDeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(4,3))\n",
    "dataset.groupby('label').headline.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "jo637O3-zaMi",
    "outputId": "db7090a8-9573-4d85-bb19-9c1a262acf9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Agree         137\n",
       "Disagree      206\n",
       "Discuss      1068\n",
       "Unrelated     586\n",
       "Name: headline, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby('label').headline.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ibRE0Lj9tSMa"
   },
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "pLuPx6X7tV8X",
    "outputId": "c9521490-6099-40d9-d0ff-761a1aa09619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanfordnlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/bf/5d2898febb6e993fcccd90484cba3c46353658511a41430012e901824e94/stanfordnlp-0.2.0-py3-none-any.whl (158kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.16.3)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.7.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (41.0.1)\n",
      "Installing collected packages: stanfordnlp\n",
      "Successfully installed stanfordnlp-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "qUA9OlQctvSR",
    "outputId": "5f1f831b-c065-422b-c9bd-f3b27f9f5912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"fa_seraji\" for language \"fa\".\n",
      "Would you like to download the models for: fa_seraji now? (Y/n)\n",
      "y\n",
      "\n",
      "Default download directory: /root/stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "\n",
      "Downloading models for: fa_seraji\n",
      "Download location: /root/stanfordnlp_resources/fa_seraji_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226M/226M [03:49<00:00, 988kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: /root/stanfordnlp_resources/fa_seraji_models.zip\n",
      "Extracting models file for: fa_seraji\n",
      "Cleaning up...Done.\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "stanfordnlp.download('fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "ygMCI-pft5qW",
    "outputId": "80a2ca5c-8b32-4441-e613-94af6eea94a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji_tokenizer.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji_lemmatizer.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,lemma', lang='fa', treebank=None, use_gpu=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "jMU7fE5c1bIh",
    "outputId": "0ecb4924-e3ee-47c9-ba26-29c42107a9ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
      "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.6/dist-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from hazm) (0.2.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3->hazm) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zdy5FjpMvUqZ"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "\n",
    "  \n",
    "k = []\n",
    "with open('stopwords.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    for word in f:\n",
    "        word = word.split('\\n')\n",
    "        k.append(word[0])\n",
    "        \n",
    "def remove_stopwords(text):\n",
    "    sw_data = []\n",
    "    for i in text:\n",
    "        for j in k:\n",
    "            if j in word_tokenize(i):\n",
    "                i.replace(j, '')\n",
    "        sw_data.append(i)\n",
    "    return sw_data\n",
    "\n",
    "\n",
    "def remove_slash(text):\n",
    "    ext_data = []\n",
    "    for i in text:\n",
    "        if '/' in i:\n",
    "            spl = i.split('/')\n",
    "            if 'شایعه' in spl[-1]:\n",
    "                i = i.replace(spl[-1], '')\n",
    "        ext_data.append(i)\n",
    "    return ext_data\n",
    "\n",
    "\n",
    "import re\n",
    "r = re.compile(\"[\\!\\;,؟:?،؛.+»«<>|\\#(\\)\\-\\/\\'\\\"]\")\n",
    "def remove_punc(text):\n",
    "    punc = []\n",
    "    for i in text:\n",
    "        punc.append(r.sub(\"\", i))\n",
    "    return punc\n",
    "\n",
    "extra_str = ['\\u200c', '\\u200d', '\\u200e', '\\u200b', '\\r', '\\n', '\\ufeff']\n",
    "def clean_data(text):\n",
    "    \n",
    "    print(\"start cleaning data..\")\n",
    "    \n",
    "    text = remove_slash(text)\n",
    "    \n",
    "    clean_data = []\n",
    "    for i in text:\n",
    "        for j in extra_str:\n",
    "            if j in i:\n",
    "                i = i.replace(j,'')\n",
    "        clean_data.append(i)\n",
    "    \n",
    "    print(\"data is ready!\")\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZk9XZU7Q8U3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "r = re.compile(\"[\\!\\;,؟:?،؛.+»«<>|\\#(\\)\\-\\/\\'\\\"]\")\n",
    "def clean(text):\n",
    "    return r.sub(\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "EOeLIjB_wfa1",
    "outputId": "0e01db64-cc6e-45be-b6b0-2a5f39c14498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start cleaning data..\n",
      "data is ready!\n"
     ]
    }
   ],
   "source": [
    "clean_claim = clean_data(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "RKhhrM922IbE",
    "outputId": "ee150aed-427d-45b3-82eb-81c28f48c5ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start cleaning data..\n",
      "data is ready!\n"
     ]
    }
   ],
   "source": [
    "clean_body = clean_data(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "M69ZHTh538Sb",
    "outputId": "5a4f72a3-4b89-4685-e42e-d61b866e2190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save clean data.\n"
     ]
    }
   ],
   "source": [
    "Dataset = list(zip(clean_claim, clean_body, label))\n",
    "    \n",
    "df = pd.DataFrame(data = Dataset, columns=['claim', 'body', 'label'])\n",
    "df.to_csv(GDRIVE_DIR / 'dataset' / 'clean_claim_body.csv', index=True, encoding=\"utf-8\")\n",
    "\n",
    "print('save clean data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fa_zU2y2vD_K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_clean = pd.read_csv(GDRIVE_DIR / 'dataset' / 'clean_claim_body.csv', index_col = 0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUhHHXho1a9n"
   },
   "outputs": [],
   "source": [
    "clean_claim = dataset_clean['claim']\n",
    "clean_body = dataset_clean['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "rJUbSCMfvUqC",
    "outputId": "04993fd0-5e2e-4ab4-9458-c1f67d9a4353"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>دستگیری سعید مرتضوی در شهر ری !</td>\n",
       "      <td>سرهنگ میررضا بهرامی دراین باره گفت: درپی وقوع ...</td>\n",
       "      <td>Unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>دستمال مرطوب حاوی مخدر ال سی من! /</td>\n",
       "      <td>تاثیر دستمال مرطوب بر آلرژی های غذایی در کودکا...</td>\n",
       "      <td>Unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>موشک عجیب ایرانی !</td>\n",
       "      <td>خبرگزاری روسی «اسپوتنیک» در مطلبی به رونمایی ی...</td>\n",
       "      <td>Discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نقل قول وزیر ارتباطات در مورد فیلتر شدن تلگرام...</td>\n",
       "      <td>به گزارش روز دوشنبه از وزارت ارتباطات و فناوری...</td>\n",
       "      <td>Discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مالکیت ستاد اجرایی فرمان حضرت امام(ره) بر مجت...</td>\n",
       "      <td>به گزارش خبرنگار حوزه اخبار داغ گروه فضای مجاز...</td>\n",
       "      <td>Discuss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  ...      label\n",
       "0                    دستگیری سعید مرتضوی در شهر ری !  ...  Unrelated\n",
       "1                 دستمال مرطوب حاوی مخدر ال سی من! /  ...  Unrelated\n",
       "2                                 موشک عجیب ایرانی !  ...    Discuss\n",
       "3  نقل قول وزیر ارتباطات در مورد فیلتر شدن تلگرام...  ...    Discuss\n",
       "4   مالکیت ستاد اجرایی فرمان حضرت امام(ره) بر مجت...  ...    Discuss\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fL2Q4zNQvpT5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = dataset_clean.label\n",
    "X = dataset_clean.drop('label', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "t_ur1L11xOJL",
    "outputId": "9c406ecd-5d44-4776-c9e4-be55add5ecbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save train data.\n"
     ]
    }
   ],
   "source": [
    "Data_train = list(zip(X_train['claim'], X_train['body'], y_train))\n",
    "    \n",
    "df = pd.DataFrame(data = Data_train, columns=['claim', 'body', 'label'])\n",
    "df.to_csv(GDRIVE_DIR / 'dataset' / 'train_data.csv', index=True, encoding=\"utf-8\")\n",
    "\n",
    "print('save train data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "atY7I1cxy_6C",
    "outputId": "10946764-2355-414a-bd65-8767b38ef1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save test data.\n"
     ]
    }
   ],
   "source": [
    "Data_test = list(zip(X_test['claim'], X_test['body'], y_test))\n",
    "    \n",
    "df = pd.DataFrame(data = Data_test, columns=['claim', 'body', 'label'])\n",
    "df.to_csv(GDRIVE_DIR / 'dataset' / 'test_data.csv', index=True, encoding=\"utf-8\")\n",
    "\n",
    "print('save test data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PvvkjOz0ynTI"
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(GDRIVE_DIR / 'dataset' / 'train_data.csv', index_col = 0, )\n",
    "data_test = pd.read_csv(GDRIVE_DIR / 'dataset' / 'test_data.csv', index_col = 0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ThVeCTtnysw-",
    "outputId": "c9538c21-7a0f-4724-cf42-bf01300b036c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1597, 400)"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train), len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5kmac9sZtlM"
   },
   "source": [
    "# Extract Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lVM3qiwYiytW"
   },
   "source": [
    "## bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SBbHZ8W6bND6"
   },
   "outputs": [],
   "source": [
    "def ngrams(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "\n",
    "def chargrams(input, n):\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "\n",
    "def append_chargrams(features, text_headline, text_body, size):\n",
    "    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(text_headline.split())), size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    grams_first_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in text_body:\n",
    "            grams_hits += 1\n",
    "        if gram in text_body[:255]:\n",
    "            grams_early_hits += 1\n",
    "        if gram in text_body[:100]:\n",
    "            grams_first_hits += 1\n",
    "    features.append(grams_hits)\n",
    "    features.append(grams_early_hits)\n",
    "    features.append(grams_first_hits)\n",
    "    return features\n",
    "\n",
    "\n",
    "def append_ngrams(features, text_headline, text_body, size):\n",
    "    grams = [' '.join(x) for x in ngrams(text_headline, size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in text_body:\n",
    "            grams_hits += 1\n",
    "        if gram in text_body[:255]:\n",
    "            grams_early_hits += 1\n",
    "    features.append(grams_hits)\n",
    "    features.append(grams_early_hits)\n",
    "    return features\n",
    "\n",
    "\n",
    "def hand_features(headlines, bodies):\n",
    "\n",
    "    def binary_co_occurence(headline, body):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text.\n",
    "        bin_count = 0\n",
    "        bin_count_early = 0\n",
    "        for headline_token in clean(headline).split(\" \"):\n",
    "            if headline_token in clean(body):\n",
    "                bin_count += 1\n",
    "            if headline_token in clean(body)[:255]:\n",
    "                bin_count_early += 1\n",
    "        return [bin_count, bin_count_early]\n",
    "\n",
    "    def binary_co_occurence_stops(headline, body):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text. Stopwords in the title\n",
    "        # are ignored.\n",
    "        bin_count = 0\n",
    "        bin_count_early = 0\n",
    "        for headline_token in remove_stopwords(clean(headline).split(\" \")):\n",
    "            if headline_token in clean(body):\n",
    "                bin_count += 1\n",
    "                bin_count_early += 1\n",
    "        return [bin_count, bin_count_early]\n",
    "\n",
    "    def count_grams(headline, body):\n",
    "        # Count how many times an n-gram of the title\n",
    "        # appears in the entire body, and intro paragraph\n",
    "\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = clean(headline)\n",
    "        features = []\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 2)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 8)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 4)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 16)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 2)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 3)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 4)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 5)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 6)\n",
    "        return features\n",
    "\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        X.append(binary_co_occurence(headline, body)\n",
    "                 + binary_co_occurence_stops(headline, body)\n",
    "                 + count_grams(headline, body))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZK6HZjTUMdM"
   },
   "outputs": [],
   "source": [
    "# x = hand_features(clean_claim, clean_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UetOVrqMzoR8"
   },
   "outputs": [],
   "source": [
    "def get_head_body_tuples(include_holdout=False):\n",
    "    # file paths\n",
    "    '''\n",
    "    data_path = \"%s/data/fnc-1\" % (path.dirname(path.dirname(path.dirname(path.abspath(__file__)))))\n",
    "    splits_dir = \"%s/data/fnc-1/splits\" % (path.dirname(path.dirname(path.dirname(path.abspath(__file__)))))\n",
    "    dataset = DataSet(data_path)\n",
    "    '''\n",
    "    data_path = myConstants.data_path\n",
    "    splits_dir = myConstants.splits_dir\n",
    "    dataset = myConstants.d\n",
    "\n",
    "    def get_stances(dataset, folds, holdout):\n",
    "        # Creates the list with a dict {'headline': ..., 'body': ..., 'stance': ...} for each\n",
    "        # stance in the data set (except for holdout)\n",
    "        stances = []\n",
    "        for stance in dataset.stances:\n",
    "            if stance['Body ID'] in holdout and include_holdout == True:\n",
    "                stances.append(stance)\n",
    "            for fold in folds:\n",
    "                if stance['Body ID'] in fold:\n",
    "                    stances.append(stance)\n",
    "\n",
    "        return stances\n",
    "\n",
    "    # create new vocabulary\n",
    "    folds, holdout = kfold_split(data_train, n_folds=10, base_dir=splits_dir)  # [[133,1334,65645,], [32323,...]] => body ids for each fold\n",
    "    stances = get_stances(dataset, folds, holdout)\n",
    "\n",
    "    print(\"Stances length: \" + str(len(stances)))\n",
    "\n",
    "    h = []\n",
    "    b = []\n",
    "    # create the final lists with all the headlines and bodies of the set except for holdout\n",
    "    for stance in stances:\n",
    "        h.append(stance['Headline'])\n",
    "        b.append(dataset.articles[stance['Body ID']])\n",
    "\n",
    "    return h, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXmoLmIlb6Z6"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_head_body_tuples_test():\n",
    "    d = myConstants.testdataset\n",
    "\n",
    "    h = []\n",
    "    b = []\n",
    "    for stance in d.stances:\n",
    "        h.append(stance['Headline'])\n",
    "        b.append(d.articles[int(stance['Body ID'])])\n",
    "\n",
    "    return h, b\n",
    "\n",
    "\n",
    "def negated_context_word_12grams_concat_tf5000_l2_all_data(headlines, bodies):\n",
    "    \"\"\"\n",
    "    Negates string after special negation word by adding a \"NEG_\" in front\n",
    "    of every negated word, until a punctuation mark appears.\n",
    "    Source:\n",
    "        NRC-Canada: Buidling the State-of-the-Art in Sentiment Analysis of Tweets\n",
    "        http://sentiment.christopherpotts.net/lingstruc.html\n",
    "        http://stackoverflow.com/questions/23384351/how-to-add-tags-to-negated-words-in-strings-that-follow-not-no-and-never\n",
    "\n",
    "\n",
    "    :param headlines:\n",
    "    :param bodies:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def get_negated_text(text):\n",
    "      sens = text.replace(';','.').replace(',','.').replace('!','.').replace(':','.').replace('،', '').split('.')\n",
    "      li_1 = ['هیچ', 'اصلا', 'هیچگونه']\n",
    "      li_2 = [ 'ندارد', 'نمیتواند']\n",
    "      jomles = []\n",
    "      for sen in sens: \n",
    "        first, second = 0 , 0\n",
    "        flag_1, flag_2 = False, False\n",
    "        tokens = word_tokenize(sen)\n",
    "        jomle = []    \n",
    "        for i in range(len(tokens)):\n",
    "          if tokens[i] in li_1 and flag_1 == False:\n",
    "            first = i\n",
    "            flag_1 = True\n",
    "          if tokens[i] in li_2 and flag_2 == False:\n",
    "            second = i\n",
    "            flag_2 = True\n",
    "        if (second > first) and (flag_1 == True) and (flag_2 == True):\n",
    "          for j in range (first + 1 , second-1 ):\n",
    "            sen = sen.replace(tokens[j], 'NEG_'+tokens[j])\n",
    "        jomles.append(sen)\n",
    "    \n",
    "      jomles = '. '.join(jomles)\n",
    "  \n",
    "      return jomles\n",
    "\n",
    "    def combine_head_and_body(headlines, bodies):\n",
    "        head_and_body = [headline + \" \" + body for i, (headline, body) in\n",
    "                         enumerate(zip(headlines, bodies))]\n",
    "\n",
    "        return head_and_body\n",
    "\n",
    "    def get_vocab(neg_headlines, neg_bodies):\n",
    "        neg_headlines = remove_stopwords(neg_headlines)\n",
    "        neg_bodies = remove_stopwords(neg_bodies)\n",
    "        tf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, use_idf=False,\n",
    "                                        norm='l2')\n",
    "        tf_vectorizer.fit_transform(combine_head_and_body(neg_headlines, neg_bodies))\n",
    "        vocab = tf_vectorizer.vocabulary_\n",
    "\n",
    "        return vocab\n",
    "\n",
    "    def get_features(neg_headlines_test, neg_bodies_test, vocab):\n",
    "        neg_headlines_test = remove_stopwords(neg_headlines_test)\n",
    "        neg_bodies_test = remove_stopwords(neg_bodies_test)\n",
    "        \n",
    "        tf_vectorizer_head = TfidfVectorizer(vocabulary=vocab, use_idf=False, norm='l2')\n",
    "        X_test_head = tf_vectorizer_head.fit_transform(neg_headlines_test)\n",
    "\n",
    "        tf_vectorizer_body = TfidfVectorizer(vocabulary=vocab, use_idf=False, norm='l2')\n",
    "        X_test_body = tf_vectorizer_body.fit_transform(neg_bodies_test)\n",
    "\n",
    "        X_test = np.concatenate([X_test_head.toarray(), X_test_body.toarray()], axis=1)\n",
    "        return X_test\n",
    "\n",
    "#     h, b = get_head_body_tuples(include_holdout=True)\n",
    "#     h_test, b_test = get_head_body_tuples_test()\n",
    "    \n",
    "    h, b = data_train['claim'].tolist() , data_train['body'].tolist()\n",
    "    h_test, b_test = data_test['claim'].tolist(), data_test['body'].tolist()\n",
    "\n",
    "    # Comment out for clean ablation tests\n",
    "    h.extend(h_test)\n",
    "    b.extend(b_test)\n",
    "\n",
    "    neg_headlines_all = [get_negated_text(h) for h in h]\n",
    "    neg_bodies_all = [get_negated_text(b) for b in b]\n",
    "    neg_headlines = [get_negated_text(h) for h in headlines]\n",
    "    neg_bodies = [get_negated_text(b) for b in bodies]\n",
    "\n",
    "    vocab = get_vocab(neg_headlines_all, neg_bodies_all)\n",
    "    X_train = get_features(neg_headlines, neg_bodies, vocab)\n",
    "\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0KlrvAVTrQ-R"
   },
   "outputs": [],
   "source": [
    "# x = negated_context_word_12grams_concat_tf5000_l2_all_data(clean_claim, clean_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqAM-p6QcGXe"
   },
   "outputs": [],
   "source": [
    "def char_3grams_5000_concat_all_data(headlines, bodies):\n",
    "\n",
    "    def combine_head_and_body(headlines, bodies):\n",
    "        return [headline + \" \" + body for i, (headline, body) in\n",
    "                tqdm(enumerate(zip(headlines, bodies)))]\n",
    "\n",
    "    # Load train data into CountVectorizer, get the resulting X-values and also the vocabulary\n",
    "    # for the test data feature creation\n",
    "    def get_features(headlines, bodies, headlines_all, bodies_all):\n",
    "        # create vocab on basis of training data\n",
    "        head_and_body = combine_head_and_body(headlines_all, bodies_all)\n",
    "        head_and_body_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), lowercase=True,\n",
    "                                              max_features=5000, use_idf=False, norm='l2')\n",
    "        head_and_body_tfidf.fit(head_and_body)\n",
    "        vocab = head_and_body_tfidf.vocabulary_\n",
    "\n",
    "        # create training feature vectors\n",
    "        X_train_head_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), lowercase=True,\n",
    "                                             stop_words='english', vocabulary=vocab, use_idf=False, norm='l2')\n",
    "        X_train_head = X_train_head_tfidf.fit_transform(headlines)\n",
    "\n",
    "        X_train_body_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), lowercase=True,\n",
    "                                             stop_words='english', vocabulary=vocab, use_idf=False, norm='l2')\n",
    "        X_train_body = X_train_body_tfidf.fit_transform(bodies)\n",
    "\n",
    "        X_train = np.concatenate([X_train_head.toarray(), X_train_body.toarray()], axis=1)\n",
    "\n",
    "        return X_train\n",
    "\n",
    "    h, b = data_train['claim'].tolist() , data_train['body'].tolist()\n",
    "    h_test, b_test = data_test['claim'].tolist(), data_test['body'].tolist()\n",
    "\n",
    "    # Comment out for clean ablation tests\n",
    "    h.extend(h_test)\n",
    "    b.extend(b_test)\n",
    "\n",
    "    X_train = get_features(headlines, bodies, h, b)\n",
    "\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Lb5fB-Eg5pB6",
    "outputId": "0323be0d-e867-4a62-d35f-d20f9a83f007"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1997it [00:00, 163179.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# x = char_3grams_5000_concat_all_data(clean_claim, clean_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "1bkNyP0i58mE",
    "outputId": "eaaa42cb-ed7b-4fd0-b6c3-9a7247ab0cd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1997, 10000)"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPN7xgRFi2V-"
   },
   "source": [
    "## embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Q8n3pQ-o4Wh"
   },
   "source": [
    "### word embedding persian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6K7iuEgJIhT"
   },
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6BQ1bFu7UBdk"
   },
   "outputs": [],
   "source": [
    "def load_embedding_pandas(FILE, type=\"w2v\"):\n",
    "  embeddings_index=dict()\n",
    "  f = open(FILE)\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "  f.close()\n",
    "  print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "  return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "dXcSLm-SUmKh",
    "outputId": "c51d1dc8-2d47-428a-c831-927aab53b93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GloVe_vectors = load_embedding_pandas(\"/content/drive/My Drive/dataset/embedding/cc.fa.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "omDwJqfYbRGU"
   },
   "outputs": [],
   "source": [
    "g_vec = pd.DataFrame.from_dict(GloVe_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5skj76c3apYu"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "FEATURES_DIR = '/content/drive/My Drive/dataset/features/'\n",
    "EMBEDDINGS_DIR = \"/content/drive/My Drive/dataset/embedding/\" \n",
    "DATA_PATH = \"/content/drive/My Drive/dataset/\"\n",
    "SPLITS_DIR = \"/content/drive/My Drive/dataset/splits/\"\n",
    "\n",
    "def create_embedding_lookup_pandas(text_list, max_nb_words, embedding_dim, embedding,\n",
    "                            embedding_lookup_name, embedding_vocab_name, rdm_emb_init=False, add_unknown=False, tokenizer=None, init_zeros = False):\n",
    "    \"\"\"\n",
    "    Creates the claim embedding lookup table if it not already exists and returns the vocabulary for it\n",
    "    :param text_list:\n",
    "    :param max_nb_words:\n",
    "    :param embedding_dim:\n",
    "    :param GloVe_vectors:\n",
    "    :param embedding_lookup_name:\n",
    "    :param embedding_vocab_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #del GloVe_vectors\n",
    "    if not path.exists(FEATURES_DIR + embedding_lookup_name) or not path.exists(FEATURES_DIR + embedding_vocab_name):\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=None, tokenizer=tokenizer,\n",
    "                                            max_features=max_nb_words, use_idf=True)\n",
    "        vectorizer.fit_transform(text_list)\n",
    "        vocab = vectorizer.vocabulary_\n",
    "\n",
    "\n",
    "        # do not use 0 since we want to use masking in the LSTM later on\n",
    "        for word in vocab.keys():\n",
    "            vocab[word] += 1\n",
    "        if add_unknown == True:\n",
    "            max_index = max(vocab.values())\n",
    "            vocab[\"UNKNOWN\"] = max_index+1\n",
    "\n",
    "        # prepare embedding - create matrix that holds the glove vector for each vocab entry\n",
    "        if rdm_emb_init == True:\n",
    "            embedding_lookup = np.random.random((len(vocab) + 1, embedding_dim))\n",
    "            zero_vec = np.zeros((embedding_dim))\n",
    "            embedding_lookup[0] = zero_vec # for masking\n",
    "        else:\n",
    "            embedding_lookup = np.zeros((len(vocab) + 1, embedding_dim))\n",
    "\n",
    "        if init_zeros == False:\n",
    "            for word, i in vocab.items():\n",
    "                if word == \"UNKNOWN\":\n",
    "                    embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
    "                    #print(embedding_vector)\n",
    "                else:\n",
    "                    try:\n",
    "                        embedding_vector = embedding.loc[word].as_matrix()\n",
    "                    except KeyError: #https://stackoverflow.com/questions/15653966/ignore-keyerror-and-continue-program\n",
    "                        continue\n",
    "                if embedding_vector is not None:\n",
    "                    # words not found in embedding index will be all-zeros.\n",
    "                    embedding_lookup[i] = embedding_vector\n",
    "        #print(embedding_lookup[-1])\n",
    "        # save embedding matrix\n",
    "        np.save(FEATURES_DIR + embedding_lookup_name, embedding_lookup)\n",
    "        # save vocab\n",
    "        with open(FEATURES_DIR + embedding_vocab_name, 'wb') as f:\n",
    "            pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print(\"Embedding lookup table shape for \" + embedding_lookup_name + \" is: \" + str(embedding_lookup.shape))\n",
    "    else:\n",
    "        with open(FEATURES_DIR + embedding_vocab_name, \"rb\") as f:\n",
    "            vocab = pickle.load(f)\n",
    "\n",
    "    print(\"Vocab size for \" + embedding_vocab_name + \" is: \" + str(len(vocab)))\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def text_to_sequences_fixed_size(texts, vocab, MAX_SENT_LENGTH, save_full_text=False, take_full_claim = False):\n",
    "    \"\"\"\n",
    "    Turns sentences of claims into sequences of indices provided by the given vocab.\n",
    "    Unknown words will get an extra index, if\n",
    "    the vocab has a token \"UNKNOWN\". The method takes the longest sentence of the claims, if the\n",
    "    claim should have more than one sentence.\n",
    "    :param texts:\n",
    "    :param vocab:\n",
    "    :param MAX_SENT_LENGTH:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = np.zeros((len(texts), MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "    claims = []\n",
    "    if take_full_claim == False:\n",
    "        for claim in texts:\n",
    "            claim_sents = nltk.sent_tokenize(claim)\n",
    "            word_count_fct = lambda sentence: len(nltk.word_tokenize(sentence)) # take longest sentence of claim if it has more than one\n",
    "            claims.append(max(claim_sents, key=word_count_fct))\n",
    "    else:\n",
    "        claims = texts\n",
    "\n",
    "    data_string_dict = {}\n",
    "    for j, claim in tqdm(enumerate(claims)):\n",
    "        claim_tokens = nltk.word_tokenize(claim.lower())\n",
    "\n",
    "        data_string = \"\"\n",
    "        if save_full_text == True:\n",
    "            for token in claim_tokens:\n",
    "                data_string += token + \" \"\n",
    "            data_string = data_string[:-1]\n",
    "            data_string_dict[j] = data_string\n",
    "\n",
    "        for i, token in enumerate(claim_tokens):\n",
    "            if i < MAX_SENT_LENGTH:\n",
    "                index = vocab.get(token, \"UNKNOWN\")\n",
    "                if index == \"UNKNOWN\":\n",
    "                    index = vocab.get(index, None)\n",
    "                if index != None:\n",
    "                    data[j, i] = index\n",
    "\n",
    "    if save_full_text == True:\n",
    "        return data, data_string_dict\n",
    "    else:\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "V9uMrbpUYE9m",
    "outputId": "5a0f22c3-33a2-4e5d-b5bd-ce8aaba89278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNEMxYF06BWf"
   },
   "outputs": [],
   "source": [
    "def single_flat_LSTM_50d_100(headlines, bodies, GloVe_vectors):\n",
    "\n",
    "    #########################\n",
    "    # PARAMETER DEFINITIONS #\n",
    "    #########################\n",
    "    method_name = \"single_flat_LSTM_50d_100\"\n",
    "    # location path for features\n",
    "    FEATURES_DIR = '/content/drive/My Drive/dataset/features/'\n",
    "    PARAM_DICT_FILENAME = method_name+\"_param_dict.pkl\"\n",
    "\n",
    "    param_dict = {\n",
    "        \"MAX_NB_WORDS\": 50000,  # size of the vocabulary\n",
    "\n",
    "        # sequence lengths\n",
    "        \"MAX_SEQ_LENGTH\": 100, #1000\n",
    "\n",
    "        # embedding specific values\n",
    "        \"EMBEDDING_DIM\": 50,  # dimension of the GloVe embeddings\n",
    "        \"GLOVE_ZIP_FILE\": \"/content/drive/My Drive/dataset/embedding/cc.fa.300.vec.gz\",  #*********************\n",
    "        \"GLOVE_FILE\": \"/content/drive/My Drive/dataset/embedding/cc.fa.300.vec\",  #*********************\n",
    "\n",
    "        # embedding file names\n",
    "        \"EMBEDDING_FILE\": method_name+\"_embedding.npy\",\n",
    "\n",
    "        # vocab file names\n",
    "        \"VOCAB_FILE\": method_name+\"_vocab.pkl\",\n",
    "    }\n",
    "\n",
    "\n",
    "    ###############################################\n",
    "    # GET VOCABULARY AND PREPARE EMBEDDING MATRIX #\n",
    "    ###############################################\n",
    "\n",
    "    # load GloVe embeddings\n",
    "    # load the whole embedding into memory\n",
    "    \n",
    "#     GloVe_vectors = load_embedding_pandas(param_dict[\"GLOVE_FILE\"])\n",
    "    \n",
    "\n",
    "    # load all claims, orig_docs and evidences\n",
    "    all_heads, all_bodies = data_train['claim'].tolist() , data_train['body'].tolist()\n",
    "    all = all_heads\n",
    "    all.extend(all_bodies)\n",
    "   \n",
    "    \n",
    "\n",
    "    # Comment out for clean ablation checks\n",
    "    # add the unlabeled test data words to the BoW of test+train+holdout data\n",
    "    h_unlbled_test, b_unlbled_test = data_test['claim'].tolist(), data_test['body'].tolist()\n",
    "    all.extend(h_unlbled_test)\n",
    "    all.extend(b_unlbled_test)\n",
    "    \n",
    "    \n",
    "    # create and save the embedding matrices for claims, orig_docs and evidences\n",
    "    vocab = create_embedding_lookup_pandas(all, param_dict[\"MAX_NB_WORDS\"], param_dict[\"EMBEDDING_DIM\"],\n",
    "                                           GloVe_vectors, param_dict[\"EMBEDDING_FILE\"], param_dict[\"VOCAB_FILE\"], init_zeros=False,\n",
    "                                           add_unknown=True, rdm_emb_init=True, tokenizer=nltk.word_tokenize)\n",
    "\n",
    "    # unload GloVe_vectors in order to make debugging possible\n",
    "    del GloVe_vectors\n",
    "\n",
    "\n",
    "    #################################################\n",
    "    # Create sequences and embedding for the claims #\n",
    "    #################################################\n",
    "    print(\"Create sequences and embedding for the heads\")\n",
    "\n",
    "    concatenated = []\n",
    "    for i in range(len(headlines)):\n",
    "        concatenated.append(headlines[i] + \". \" + bodies[i])\n",
    "\n",
    "    # replace tokens of claims by vocabulary ids - the ids refer to the index of the embedding matrix which holds the word embedding for this vocab word\n",
    "    sequences = text_to_sequences_fixed_size(concatenated, vocab, param_dict[\"MAX_SEQ_LENGTH\"], save_full_text=False,\n",
    "                                             take_full_claim=True)\n",
    "\n",
    "\n",
    "\n",
    "    #################################################\n",
    "    # SAVE PARAM_DICT AND CONCATENATE TRAINING DATA #\n",
    "    #################################################\n",
    "\n",
    "    # save param_dict\n",
    "    with open(FEATURES_DIR+PARAM_DICT_FILENAME, 'wb') as f:\n",
    "        pickle.dump(param_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Save PARAM_DICT as \" + FEATURES_DIR+PARAM_DICT_FILENAME)\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "0iYxaWtCXjCF",
    "outputId": "c93dda92-cb2e-4369-d0ac-da86e82a1d3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 3164.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for single_flat_LSTM_50d_100_vocab.pkl is: 48873\n",
      "Create sequences and embedding for the heads\n",
      "Save PARAM_DICT as /content/drive/My Drive/dataset/features/single_flat_LSTM_50d_100_param_dict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# aa = single_flat_LSTM_50d_100(docs, docs_2, g_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiD6GBFmr-aK"
   },
   "outputs": [],
   "source": [
    "# !gunzip \"/content/drive/My Drive/dataset/embedding/cc.fa.300.vec.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHFR3Mqyi-EF"
   },
   "source": [
    "## generate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "URLSI-k5h2OH",
    "outputId": "7b9904a6-d21e-45b4-aab0-244920ed8d7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim', 'body', 'label'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_Pz8dQUbElO"
   },
   "outputs": [],
   "source": [
    "bow_feats = ['hand', 'negated_context_word_12grams_concat_tf5000_l2_all_data', 'char_3grams_5000_concat_all_data']\n",
    "\n",
    "word_emb = ['single_flat_LSTM_50d_100']\n",
    "\n",
    "#topic_models = ['latent_dirichlet_allocation_300', 'latent_semantic_indexing_gensim_300_concat', 'NMF_fit_all_concat_300_no_holdout', 'NMF_cos_300']\n",
    "# topic_models = ['latent_dirichlet_allocation_300', 'latent_semantic_indexing_gensim_300_concat_holdout', 'NMF_fit_all_concat_300_and_test', 'NMF_cos_300']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "etKS3L3WhF58"
   },
   "outputs": [],
   "source": [
    "def gen_or_load_feats(feat_fn, headlines, bodies, feature_file, feature):\n",
    "    if not os.path.isfile(feature_file):\n",
    "        if 'single_flat_LSTM_50d_100' in feature:\n",
    "            feats = feat_fn(headlines, bodies, g_vec)\n",
    "        else:\n",
    "            feats = feat_fn(headlines, bodies)\n",
    "        np.save(feature_file, feats)\n",
    "\n",
    "    return np.load(feature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ohJHd6qgCDt"
   },
   "outputs": [],
   "source": [
    "def generate_features(dataset, name, feature_list, features_dir):\n",
    "    \"\"\"\n",
    "    Creates feature vectors out of the provided dataset\n",
    "    \"\"\"\n",
    "    h, b, y = [], [], []\n",
    "\n",
    "    feature_dict = {'hand': hand_features,\n",
    "                    'single_flat_LSTM_50d_100': single_flat_LSTM_50d_100,\n",
    "                    'char_3grams_5000_concat_all_data': char_3grams_5000_concat_all_data,\n",
    "                    'negated_context_word_12grams_concat_tf5000_l2_all_data': negated_context_word_12grams_concat_tf5000_l2_all_data,\n",
    "                    }\n",
    "    \n",
    "    y = dataset['label'].tolist()\n",
    "    h = dataset['claim'].tolist()\n",
    "    b = dataset['body'].tolist()\n",
    "\n",
    "    X_feat = []\n",
    "    feat_list = []\n",
    "    last_index = 0\n",
    "    for feature in feature_list:\n",
    "        feat = gen_or_load_feats(feature_dict[feature], h, b, features_dir+\"/\"+feature+\".\"+name+'.npy', feature)\n",
    "        feat_list.append((last_index, last_index+len(feat[0]), str(feature)))\n",
    "        last_index += len(feat[0])\n",
    "        X_feat.append(feat)\n",
    "    X = np.concatenate(X_feat, axis=1)\n",
    "\n",
    "    return X, y, feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWezqPAPjeva"
   },
   "outputs": [],
   "source": [
    "features_dir = '/content/drive/My Drive/dataset/features'\n",
    "feature_list = word_emb + bow_feats\n",
    "name = 'first_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "eyBjbFoBhiMw",
    "outputId": "43946467-b0be-4c24-f562-2d94f653b713"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:00, 278.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding lookup table shape for single_flat_LSTM_50d_100_embedding.npy is: (48874, 50)\n",
      "Vocab size for single_flat_LSTM_50d_100_vocab.pkl is: 48873\n",
      "Create sequences and embedding for the heads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1997it [00:07, 251.74it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save PARAM_DICT as /content/drive/My Drive/dataset/features/single_flat_LSTM_50d_100_param_dict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1997it [10:56,  3.39it/s]\n",
      "1997it [00:00, 302996.13it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y , feature_list = generate_features(dataset_clean, name, feature_list, features_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "Vsw4tRMW2Dhh",
    "outputId": "31ae2d3a-9a3d-4bc7-e2a0-50011adfe072"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 100, 'single_flat_LSTM_50d_100'),\n",
       " (100, 126, 'hand'),\n",
       " (126, 10126, 'negated_context_word_12grams_concat_tf5000_l2_all_data'),\n",
       " (10126, 20126, 'char_3grams_5000_concat_all_data')]"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r69G5MZ-ZyNs"
   },
   "source": [
    "# Base lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d7M8JN4WaYwz"
   },
   "source": [
    "majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtXP3tKXYlGF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset_clean['claim'], dataset_clean['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EppwBAJrXEMu"
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf_maj = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "clf_maj.fit(X_train, y_train)\n",
    "\n",
    "y_pred_maj = clf_maj.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "TW1IOOYwZKLi",
    "outputId": "4ffe03a9-13d6-414a-c00f-bb25942c66de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNC-1 score from restored model: 0.6391129032258065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score, cm = score_submission(y_test, y_pred_maj)\n",
    "fold_score, _ = score_submission(y_test, y_pred_maj)\n",
    "max_fold_score, _ = score_submission(y_test, y_test)\n",
    "score = fold_score / max_fold_score\n",
    "\n",
    "print(\"FNC-1 score from restored model: \" +  str(score) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "oIn8-WFuYC3B",
    "outputId": "0a2b3b48-ca25-4594-e1c4-559466b55295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   Agree   | Disagree  |  Discuss  | Unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   Agree   |     0     |     0     |    32     |     0     |\n",
      "-------------------------------------------------------------\n",
      "| Disagree  |     0     |     0     |    35     |     0     |\n",
      "-------------------------------------------------------------\n",
      "|  Discuss  |     0     |     0     |    221    |     0     |\n",
      "-------------------------------------------------------------\n",
      "| Unrelated |     0     |     0     |    112    |     0     |\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrJQpoP8Z1xx"
   },
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jm7gN_tDXQpF"
   },
   "outputs": [],
   "source": [
    "def split_X(X_train, MAX_SEQ_LENGTH_HEADS):\n",
    "    # split to get [heads, docs]\n",
    "    X_train_splits = np.hsplit(X_train, np.array([MAX_SEQ_LENGTH_HEADS]))\n",
    "    X_train_head = X_train_splits[0]\n",
    "    X_train_doc = X_train_splits[1]\n",
    "\n",
    "    print(\"X_train_head.shape = \" + str(np.array(X_train_head).shape))\n",
    "    print(\"X_train_doc.shape = \" + str(np.array(X_train_doc).shape))\n",
    "\n",
    "    return X_train_head,X_train_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CxDn3uG42KQD",
    "outputId": "bf9288af-6a69-4394-a14f-b3689bbc1c66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os.path as path\n",
    "import pickle\n",
    "\n",
    "from keras.layers.core import Dense\n",
    "# from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import optimizers\n",
    "# from fnc.models.Keras_utils import EarlyStoppingOnF1, convert_data_to_one_hot, calculate_class_weight, split_X\n",
    "# from fnc.models.keras_custom_layers.attention_custom import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Embedding, Input\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BV_gZl423WL0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kphZuuiS7xMe"
   },
   "outputs": [],
   "source": [
    "def convert_data_to_one_hot(y_train):\n",
    "    \n",
    "    y_train_temp = np.zeros((y_train.size, y_train.max() + 1), dtype=np.int)\n",
    "    y_train_temp[np.arange(y_train.size), y_train] = 1\n",
    "\n",
    "    return y_train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "05-6Ix2L8EkU",
    "outputId": "b29421e9-dd1a-4cc0-b8e4-8eabf6b9c1da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agree', 'Disagree', 'Discuss', 'Unrelated']\n",
      "['Agree', 'Disagree', 'Discuss', 'Unrelated']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import keras\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "print(list(le.classes_))\n",
    "y_train = le.transform(y_train)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_test)\n",
    "print(list(le.classes_))\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNNYZbzY5Jjs"
   },
   "outputs": [],
   "source": [
    "def calculate_class_weight(y_train, no_classes=2):\n",
    "    # https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n",
    "    from sklearn.utils import class_weight\n",
    "\n",
    "    class_weight_list = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "    class_weights = {}\n",
    "    for i in range(no_classes):\n",
    "        class_weights[i] = class_weight_list[i]\n",
    "    print(class_weights)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "K3K_ngFR2Obp",
    "outputId": "2c25e7c0-cf2e-4ea4-ee33-9d5d02a5dd93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_head.shape = (1597, 100)\n",
      "X_train_doc.shape = (1597, 20026)\n",
      "X_train_head.shape = (400, 100)\n",
      "X_train_doc.shape = (400, 20026)\n",
      "EMBEDDING_FILE.shape = (48874, 50)\n",
      "{0: 3.802380952380952, 1: 2.334795321637427, 2: 0.47136953955135774, 3: 0.8422995780590717}\n"
     ]
    }
   ],
   "source": [
    "y_train_one_hot = convert_data_to_one_hot(y_train)\n",
    "y_test_one_hot = convert_data_to_one_hot(y_test)\n",
    "\n",
    "param_dict=\"single_flat_LSTM_50d_100\"\n",
    "\n",
    "FEATURES_DIR = '/content/drive/My Drive/dataset/features/'\n",
    "\n",
    "PARAM_DICT_FILENAME = param_dict + \"_param_dict.pkl\"\n",
    "\n",
    "\n",
    "# load feature dict for LSTM_1000_GloVe\n",
    "with open(FEATURES_DIR+PARAM_DICT_FILENAME, \"rb\") as f:\n",
    "  param_dict = pickle.load(f)\n",
    "\n",
    "# load parameters needed for embedding layer\n",
    "EMBEDDING_DIM = param_dict[\"EMBEDDING_DIM\"] # e.g. 50\n",
    "MAX_SEQ_LENGTH = param_dict[\"MAX_SEQ_LENGTH\"] # e.g. 100\n",
    "\n",
    "X_train_LSTM, X_train_MLP = split_X(X_train, MAX_SEQ_LENGTH)\n",
    "X_test_LSTM, X_test_MLP = split_X(X_test, MAX_SEQ_LENGTH)\n",
    "\n",
    "# load embeddings\n",
    "EMBEDDING_FILE = np.load(FEATURES_DIR+param_dict[\"EMBEDDING_FILE\"])\n",
    "\n",
    "print(\"EMBEDDING_FILE.shape = \" + str(EMBEDDING_FILE.shape))\n",
    "\n",
    "# calc cass weights\n",
    "class_weights = calculate_class_weight(y_train, no_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "colab_type": "code",
    "id": "m2ZdZpGm5TRs",
    "outputId": "12c358a5-56c5-4848-800c-8a0e828f8803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 100, 50)      2443700     lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100, 100)     60400       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100)          80400       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mlp_input (InputLayer)          (None, 20026)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20126)        0           lstm_2[0][0]                     \n",
      "                                                                 mlp_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 600)          12076200    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 600)          360600      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 600)          360600      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_out (Dense)               (None, 4)            2404        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,384,304\n",
      "Trainable params: 15,384,304\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "kernel_initializer = 'glorot_uniform'\n",
    "regularizer = None\n",
    "batch_size=200\n",
    "dense_activity_regularizer=None\n",
    "LSTM_implementation = 2\n",
    "\n",
    "\n",
    "################\n",
    "# CLAIMS LAYER #\n",
    "################\n",
    "lstm_input = Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name='lstm_input') # receive sequences of MAX_SEQ_LENGTH_CLAIMS integers\n",
    "embedding = Embedding(input_dim=len(EMBEDDING_FILE), # lookup table size\n",
    "                                    output_dim=EMBEDDING_DIM, # output dim for each number in a sequence\n",
    "                                    weights=[EMBEDDING_FILE],\n",
    "                                    input_length=MAX_SEQ_LENGTH, # receive sequences of MAX_SEQ_LENGTH_CLAIMS integers\n",
    "                                    mask_zero=True,\n",
    "                                    trainable=True)(lstm_input)\n",
    "data_LSTM = LSTM(\n",
    "            100, return_sequences=True, stateful=False, dropout=0.2,\n",
    "            batch_input_shape=(batch_size, MAX_SEQ_LENGTH, EMBEDDING_DIM),\n",
    "            input_shape=(MAX_SEQ_LENGTH, EMBEDDING_DIM), implementation=LSTM_implementation\n",
    "            )(embedding)\n",
    "data_LSTM = LSTM(\n",
    "            100, return_sequences=False, stateful=False, dropout=0.2,\n",
    "            batch_input_shape=(batch_size, MAX_SEQ_LENGTH, EMBEDDING_DIM),\n",
    "            input_shape=(MAX_SEQ_LENGTH, EMBEDDING_DIM), implementation=LSTM_implementation\n",
    "            )(data_LSTM)\n",
    "\n",
    "###############################\n",
    "# MLP (NON-TIMESTEP) FEATURES #\n",
    "###############################\n",
    "mlp_input = Input(shape=(len(X_train_MLP[0]),), dtype='float32', name='mlp_input')\n",
    "\n",
    "###############\n",
    "# MERGE LAYER #\n",
    "###############\n",
    "merged = concatenate([data_LSTM, mlp_input])\n",
    "\n",
    "dense_mid = Dense(600, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n",
    "                          activity_regularizer=dense_activity_regularizer, activation='relu')(merged)\n",
    "dense_mid = Dense(600, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n",
    "                          activity_regularizer=dense_activity_regularizer, activation='relu')(dense_mid)\n",
    "dense_mid = Dense(600, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n",
    "                          activity_regularizer=dense_activity_regularizer, activation='relu')(dense_mid)\n",
    "dense_out = Dense(4,activation='softmax', name='dense_out')(dense_mid)\n",
    "\n",
    "# build model\n",
    "model = Model(inputs=[lstm_input, mlp_input], outputs=[dense_out])\n",
    "\n",
    "# print summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qVH4fd1KDJzm"
   },
   "outputs": [],
   "source": [
    "LABELS = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n",
    "LABELS_RELATED = ['unrelated','related']\n",
    "RELATED = LABELS[0:3]\n",
    "\n",
    "def score_submission(gold_labels, test_labels):\n",
    "    score = 0.0\n",
    "    cm = [[0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0]]\n",
    "\n",
    "    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n",
    "        g_stance, t_stance = g, t\n",
    "        if g_stance == t_stance:\n",
    "            score += 0.25\n",
    "            if g_stance != 'unrelated':\n",
    "                score += 0.50\n",
    "        if g_stance in RELATED and t_stance in RELATED:\n",
    "            score += 0.25\n",
    "\n",
    "        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n",
    "\n",
    "    return score, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzm0UREZC3fN"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "class EarlyStoppingOnF1(Callback):\n",
    "    \"\"\"\n",
    "    Prints some metrics after each epoch in order to observe overfitting\n",
    "                https://github.com/fchollet/keras/issues/5794\n",
    "                custom metrics: https://github.com/fchollet/keras/issues/2607\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epochs,\n",
    "                 X_test_claims,\n",
    "                 X_test_orig_docs,\n",
    "                 y_test, loss_filename, epsilon=0.0, min_epoch = 15, X_test_nt=None):\n",
    "        self.epochs = epochs\n",
    "        self.patience = 2\n",
    "        self.counter = 0\n",
    "        self.prev_score = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.loss_filename = loss_filename\n",
    "        self.min_epoch = min_epoch\n",
    "        self.X_test_nt = X_test_nt\n",
    "        #self.print_train_f1 = print_train_f1\n",
    "\n",
    "        #self.X_train_claims = X_train_claims\n",
    "        #self.X_train_orig_docs = X_train_orig_docs\n",
    "        #self.X_train_evid = X_train_evid\n",
    "        #self.y_train = y_train\n",
    "\n",
    "        self.X_test_claims = X_test_claims\n",
    "        self.X_test_orig_docs = X_test_orig_docs\n",
    "        self.y_test = y_test\n",
    "        Callback.__init__(self)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch + 1 < self.epochs:\n",
    "            from sklearn.metrics import f1_score\n",
    "\n",
    "            # get prediction and convert into list\n",
    "            if type(self.X_test_orig_docs).__module__ == np.__name__ and type(self.X_test_nt).__module__ == np.__name__:\n",
    "                predicted_one_hot = self.model.predict([\n",
    "                    self.X_test_claims,\n",
    "                    self.X_test_orig_docs,\n",
    "                    self.X_test_nt\n",
    "                ])\n",
    "            elif type(self.X_test_orig_docs).__module__ == np.__name__:\n",
    "                predicted_one_hot = self.model.predict([\n",
    "                    self.X_test_claims,\n",
    "                    self.X_test_orig_docs,\n",
    "                ])\n",
    "            else:\n",
    "                predicted_one_hot = self.model.predict(self.X_test_claims)\n",
    "            predict = np.argmax(predicted_one_hot, axis=-1)\n",
    "\n",
    "            \"\"\"\n",
    "            predicted_one_hot_train = self.model.predict([self.X_train_claims, self.X_train_orig_docs, self.X_train_evid])\n",
    "            predict_train = np.argmax(predicted_one_hot_train, axis=-1)\n",
    "\n",
    "            \n",
    "            # f1 for train data\n",
    "            f1_macro_train = \"\"\n",
    "            if self.print_train_f1 == True:\n",
    "                f1_0_train = f1_score(self.y_train, predict_train, labels=[0], average=None)\n",
    "                f1_1_train = f1_score(self.y_train, predict_train, labels=[1], average=None)\n",
    "                f1_macro_train = (f1_0_train[0] + f1_1_train[0]) / 2\n",
    "                print(\" - train_f1_(macro): \" + str(f1_macro_train))\"\"\"\n",
    "\n",
    "            predicted = [LABELS[int(a)] for a in predict]\n",
    "            actual = [LABELS[int(a)] for a in self.y_test]\n",
    "            # calc FNC score\n",
    "            fold_score, _ = score_submission(actual, predicted)\n",
    "            max_fold_score, _ = score_submission(actual, actual)\n",
    "            fnc_score = fold_score / max_fold_score\n",
    "            print(\" - fnc_score: \" + str(fnc_score))\n",
    "\n",
    "            # f1 for test data\n",
    "            f1_0 = f1_score(self.y_test, predict, labels=[0], average=None)\n",
    "            f1_1 = f1_score(self.y_test, predict, labels=[1], average=None)\n",
    "            f1_2 = f1_score(self.y_test, predict, labels=[2], average=None)\n",
    "            f1_3 = f1_score(self.y_test, predict, labels=[3], average=None)\n",
    "            f1_macro = (f1_0[0] + f1_1[0] + f1_2[0] + f1_3[0]) / 4\n",
    "            print(\" - val_f1_(macro): \" + str(f1_macro))\n",
    "            print(\"\\n\")\n",
    "\n",
    "            header = \"\"\n",
    "            values = \"\"\n",
    "            for key, value in logs.items():\n",
    "                header = header + key + \";\"\n",
    "                values = values + str(value) + \";\"\n",
    "            if epoch == 0:\n",
    "                values = \"\\n\" + header + \"val_f1_macro;\" + \"fnc_score;\" + \"\\n\" + values + str(f1_macro) + str(fnc_score) + \";\"\n",
    "            else:\n",
    "                values += str(f1_macro) + \";\" + str(fnc_score) + \";\"\n",
    "            append_to_loss_monitor_file(values, self.loss_filename)\n",
    "\n",
    "            if epoch >= self.min_epoch-1:  # 9\n",
    "                if f1_macro + self.epsilon <= self.prev_score:\n",
    "                    self.counter += 1\n",
    "                else:\n",
    "                    self.counter = 0\n",
    "                if self.counter >= 2:\n",
    "                    self.model.stop_training = True\n",
    "            #print(\"Counter at \" + str(self.counter))\n",
    "            self.prev_score = f1_macro\n",
    "            #print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubk3uR3FF87-"
   },
   "outputs": [],
   "source": [
    "def append_to_loss_monitor_file(text, filepath):\n",
    "    with open(filepath, 'a+') as the_file:\n",
    "        the_file.write(text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2692
    },
    "colab_type": "code",
    "id": "Xn-ek0kr-wPk",
    "outputId": "2f6fe063-2495-4d6d-b72b-0ade8c742e37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used optimizer: rms, lr=0.001\n",
      "Train on 1597 samples, validate on 400 samples\n",
      "Epoch 1/70\n",
      "1597/1597 [==============================] - 20s 12ms/step - loss: 1.8626 - acc: 0.6832 - val_loss: 1.0774 - val_acc: 0.6600\n",
      " - fnc_score: 0.696236559139785\n",
      " - val_f1_(macro): 0.5974132985180051\n",
      "\n",
      "\n",
      "Epoch 2/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0817 - acc: 0.9618 - val_loss: 1.2092 - val_acc: 0.6725\n",
      " - fnc_score: 0.7110215053763441\n",
      " - val_f1_(macro): 0.5957721912431815\n",
      "\n",
      "\n",
      "Epoch 3/70\n",
      "1597/1597 [==============================] - 16s 10ms/step - loss: 0.1066 - acc: 0.9543 - val_loss: 2.8635 - val_acc: 0.3975\n",
      " - fnc_score: 0.478494623655914\n",
      " - val_f1_(macro): 0.4312379712717813\n",
      "\n",
      "\n",
      "Epoch 4/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.2208 - acc: 0.9123 - val_loss: 1.2807 - val_acc: 0.6825\n",
      " - fnc_score: 0.7123655913978495\n",
      " - val_f1_(macro): 0.5979179537531442\n",
      "\n",
      "\n",
      "Epoch 5/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0260 - acc: 0.9894 - val_loss: 1.5948 - val_acc: 0.6675\n",
      " - fnc_score: 0.6881720430107527\n",
      " - val_f1_(macro): 0.5910469500610571\n",
      "\n",
      "\n",
      "Epoch 6/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.6632 - acc: 0.8096 - val_loss: 1.0171 - val_acc: 0.6700\n",
      " - fnc_score: 0.7083333333333334\n",
      " - val_f1_(macro): 0.5825290460789375\n",
      "\n",
      "\n",
      "Epoch 7/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0444 - acc: 0.9900 - val_loss: 1.3120 - val_acc: 0.6800\n",
      " - fnc_score: 0.7157258064516129\n",
      " - val_f1_(macro): 0.5893772048439584\n",
      "\n",
      "\n",
      "Epoch 8/70\n",
      "1597/1597 [==============================] - 17s 11ms/step - loss: 0.0162 - acc: 0.9944 - val_loss: 1.5908 - val_acc: 0.6925\n",
      " - fnc_score: 0.7271505376344086\n",
      " - val_f1_(macro): 0.591326506159934\n",
      "\n",
      "\n",
      "Epoch 9/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0132 - acc: 0.9944 - val_loss: 1.6643 - val_acc: 0.6725\n",
      " - fnc_score: 0.7137096774193549\n",
      " - val_f1_(macro): 0.5705915957602532\n",
      "\n",
      "\n",
      "Epoch 10/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.6461 - acc: 0.8109 - val_loss: 1.1204 - val_acc: 0.6825\n",
      " - fnc_score: 0.7231182795698925\n",
      " - val_f1_(macro): 0.5899082598711498\n",
      "\n",
      "\n",
      "Epoch 11/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0288 - acc: 0.9912 - val_loss: 1.4264 - val_acc: 0.6725\n",
      " - fnc_score: 0.7096774193548387\n",
      " - val_f1_(macro): 0.5769328667932416\n",
      "\n",
      "\n",
      "Epoch 12/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0103 - acc: 0.9962 - val_loss: 1.8475 - val_acc: 0.6925\n",
      " - fnc_score: 0.7278225806451613\n",
      " - val_f1_(macro): 0.5584697670715499\n",
      "\n",
      "\n",
      "Epoch 13/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0687 - acc: 0.9718 - val_loss: 1.6268 - val_acc: 0.6900\n",
      " - fnc_score: 0.7264784946236559\n",
      " - val_f1_(macro): 0.576943855219465\n",
      "\n",
      "\n",
      "Epoch 14/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0091 - acc: 0.9969 - val_loss: 1.9772 - val_acc: 0.7075\n",
      " - fnc_score: 0.7432795698924731\n",
      " - val_f1_(macro): 0.5800103730336288\n",
      "\n",
      "\n",
      "Epoch 15/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0050 - acc: 0.9987 - val_loss: 2.0668 - val_acc: 0.7050\n",
      " - fnc_score: 0.7405913978494624\n",
      " - val_f1_(macro): 0.5906185504942865\n",
      "\n",
      "\n",
      "Epoch 16/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0095 - acc: 0.9950 - val_loss: 2.3434 - val_acc: 0.6900\n",
      " - fnc_score: 0.728494623655914\n",
      " - val_f1_(macro): 0.5492965698731543\n",
      "\n",
      "\n",
      "Epoch 17/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.5727 - acc: 0.8190 - val_loss: 1.0419 - val_acc: 0.6900\n",
      " - fnc_score: 0.730510752688172\n",
      " - val_f1_(macro): 0.6051055993031509\n",
      "\n",
      "\n",
      "Epoch 18/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0295 - acc: 0.9881 - val_loss: 1.5112 - val_acc: 0.7075\n",
      " - fnc_score: 0.7426075268817204\n",
      " - val_f1_(macro): 0.5977769807711668\n",
      "\n",
      "\n",
      "Epoch 19/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0080 - acc: 0.9969 - val_loss: 1.8486 - val_acc: 0.7050\n",
      " - fnc_score: 0.7399193548387096\n",
      " - val_f1_(macro): 0.5784139668689753\n",
      "\n",
      "\n",
      "Epoch 20/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0039 - acc: 0.9981 - val_loss: 2.0681 - val_acc: 0.7050\n",
      " - fnc_score: 0.741263440860215\n",
      " - val_f1_(macro): 0.5724395604983812\n",
      "\n",
      "\n",
      "Epoch 21/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.7447 - acc: 0.8541 - val_loss: 1.0545 - val_acc: 0.6425\n",
      " - fnc_score: 0.6821236559139785\n",
      " - val_f1_(macro): 0.5810431920693323\n",
      "\n",
      "\n",
      "Epoch 22/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0466 - acc: 0.9831 - val_loss: 1.4198 - val_acc: 0.6800\n",
      " - fnc_score: 0.7157258064516129\n",
      " - val_f1_(macro): 0.5624016186609364\n",
      "\n",
      "\n",
      "Epoch 23/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0101 - acc: 0.9969 - val_loss: 1.7187 - val_acc: 0.6875\n",
      " - fnc_score: 0.717741935483871\n",
      " - val_f1_(macro): 0.5999086677663106\n",
      "\n",
      "\n",
      "Epoch 24/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0059 - acc: 0.9981 - val_loss: 1.9403 - val_acc: 0.6925\n",
      " - fnc_score: 0.7251344086021505\n",
      " - val_f1_(macro): 0.5993408444485145\n",
      "\n",
      "\n",
      "Epoch 25/70\n",
      "1597/1597 [==============================] - 17s 10ms/step - loss: 0.0051 - acc: 0.9975 - val_loss: 2.1617 - val_acc: 0.6925\n",
      " - fnc_score: 0.7264784946236559\n",
      " - val_f1_(macro): 0.5927288836550508\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "optimizer_name = \"rms\"\n",
    "use_class_weights = True\n",
    "epochs = 70\n",
    "min_epoch = 20\n",
    "save_folder = None\n",
    "loss_filename = 'loss.h5'\n",
    "\n",
    "# optimizers\n",
    "if optimizer_name == \"adagrad\":\n",
    "  optimizer = optimizers.Adagrad(lr=lr)\n",
    "  print(\"Used optimizer: adagrad, lr=\"+str(lr))\n",
    "elif optimizer_name == \"adamax\":\n",
    "  optimizer = optimizers.Adamax(lr=lr)\n",
    "  print(\"Used optimizer: adamax, lr=\"+str(lr))\n",
    "elif optimizer_name == \"nadam\":\n",
    "  optimizer = optimizers.Nadam(lr=lr)  # recommended to leave at default params\n",
    "  print(\"Used optimizer: nadam, lr=\"+str(lr))\n",
    "elif optimizer_name == \"rms\":\n",
    "  optimizer = optimizers.RMSprop(lr=lr)  # recommended for RNNs\n",
    "  print(\"Used optimizer: rms, lr=\"+str(lr))\n",
    "elif optimizer_name == \"SGD\":\n",
    "  optimizer = optimizers.SGD(lr=lr)  # recommended for RNNs\n",
    "  print(\"Used optimizer: SGD, lr=\"+str(lr))\n",
    "elif optimizer_name == \"adadelta\":\n",
    "  optimizer = optimizers.Adadelta(lr)  # recommended for RNNs\n",
    "  print(\"Used optimizer: adadelta, lr=\"+str(lr))\n",
    "else:\n",
    "  optimizer = optimizers.Adam(lr=lr)\n",
    "  print(\"Used optimizer: Adam, lr=\" + str(lr))\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer, 'kullback_leibler_divergence', # categorial_crossentropy\n",
    "                           metrics=['accuracy'])\n",
    "if use_class_weights == True:\n",
    "  hist = model.fit([X_train_LSTM, X_train_MLP],\n",
    "                             y_train_one_hot,\n",
    "                             validation_data=([X_test_LSTM, X_test_MLP], y_test_one_hot),\n",
    "                             batch_size=batch_size, epochs=epochs, verbose=1, class_weight=class_weights,\n",
    "                              callbacks=[\n",
    "                               EarlyStoppingOnF1(epochs,\n",
    "                                                 X_test_LSTM, X_test_MLP, y_test,\n",
    "                                                 loss_filename, epsilon=0.0, min_epoch=min_epoch),])\n",
    "else:\n",
    "  hist = model.fit([X_train_LSTM, X_train_MLP],\n",
    "                             y_train_one_hot,\n",
    "                             validation_data=([X_test_LSTM, X_test_MLP], y_test_one_hot),\n",
    "                             batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                           callbacks=[\n",
    "                               EarlyStoppingOnF1(epochs,\n",
    "                                                 X_test_LSTM, X_test_MLP, y_test,\n",
    "                                                 loss_filename, epsilon=0.0, min_epoch=min_epoch),])\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIzK9drzHIyM"
   },
   "outputs": [],
   "source": [
    "model.save(\"save.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jETm9GZSL25i"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/My Drive/dataset/models/model_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "WV9kszEgJgH9",
    "outputId": "59235591-886d-4df0-c2eb-487544877f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from:save.h5\n",
      "X_train_head.shape = (400, 100)\n",
      "X_train_doc.shape = (400, 20026)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model from:\" + \"save.h5\")\n",
    "model = load_model(\"save.h5\")\n",
    "if (model != None):\n",
    "  X_test_LSTM, X_test_MLP = split_X(X_test, MAX_SEQ_LENGTH)\n",
    "predicted_one_hot = model.predict([X_test_LSTM, X_test_MLP])\n",
    "predicted_int = np.argmax(predicted_one_hot, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-rvnwzcrJ_xc"
   },
   "outputs": [],
   "source": [
    "y_test_str = le.inverse_transform(y_test)\n",
    "y_pred_str = le.inverse_transform(predicted_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ChlIVSHMKeRe",
    "outputId": "55e6121f-6001-44d9-d23f-1150842cee71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNC-1 score from restored model: 0.7264784946236559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score, cm = score_submission(y_test_str, y_pred_str)\n",
    "fold_score, _ = score_submission(y_test_str, y_pred_str)\n",
    "max_fold_score, _ = score_submission(y_test_str, y_test_str)\n",
    "score = fold_score / max_fold_score\n",
    "\n",
    "print(\"FNC-1 score from restored model: \" +  str(score) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zlb4p0aCKtIy"
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(cm):\n",
    "    lines = []\n",
    "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
    "    line_len = len(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "    lines.append(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    for i, row in enumerate(cm):\n",
    "        hit += row[i]\n",
    "        total += sum(row)\n",
    "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n",
    "                                                                   *row))\n",
    "        lines.append(\"-\"*line_len)\n",
    "    print('\\n'.join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "XYIs6CZmKyAc",
    "outputId": "ea6cb1a9-e2eb-4d2f-c453-76d083e66c19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   Agree   | Disagree  |  Discuss  | Unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   Agree   |    10     |     1     |    17     |     4     |\n",
      "-------------------------------------------------------------\n",
      "| Disagree  |     2     |    17     |    14     |     2     |\n",
      "-------------------------------------------------------------\n",
      "|  Discuss  |     5     |     7     |    177    |    32     |\n",
      "-------------------------------------------------------------\n",
      "| Unrelated |     0     |     2     |    37     |    73     |\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(cm)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VAvZfSsgZoRd",
    "ibRE0Lj9tSMa",
    "u5kmac9sZtlM",
    "YrJQpoP8Z1xx"
   ],
   "name": "DM_f2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
