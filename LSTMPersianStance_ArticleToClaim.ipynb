{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_PersianStance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAgNOJBVVMQ_"
      },
      "source": [
        "# Persian Stance Classification - Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMgbaBPU5wuC"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import datetime\n",
        "import argparse\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import os.path as path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAvZfSsgZoRd"
      },
      "source": [
        "# Read Data set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pvBMbjyZ7EN"
      },
      "source": [
        "**claim and body**\n",
        "\n",
        "```\n",
        "1997\n",
        "\n",
        "748\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eNNmTGnZb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "4c7f66f3-b3a4-42ba-de7c-133a85c19f2c"
      },
      "source": [
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "GDRIVE_DIR = Path(\"/content/drive/My Drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjBiHamk6v4o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "485689f9-e21c-452f-c814-3bf789065737"
      },
      "source": [
        "import glob\n",
        "\n",
        "# txt to CSV\n",
        "\n",
        "data_path = \"/content/drive/My Drive/persian_stance_deep_data/dataset/ArticleToClaim.txt\"\n",
        "csv_path = \"/content/drive/My Drive/persian_stance_deep_data/dataset/ArticleToClaim.csv\"\n",
        "\n",
        "data_file = glob.glob(data_path)\n",
        "\n",
        "row_documents = []\n",
        "cnt = 1\n",
        "\n",
        "for file in data_file:\n",
        "    with open (file, \"r\", encoding=\"utf-8\") as fp:\n",
        "        line = fp.readline() #first line is for the headers (Claim, Body Text, Claim Is Question, Claim Has Tow Parts, Stance)\n",
        "        content = fp.read() #read the rest of the file to a string\n",
        "\n",
        "row_documents = content.split(\"#@@@@@#\") #split the instances by the custom delimiter\n",
        "row_documents = list(filter(None, row_documents)) #remove empty instance\n",
        "print(\"number of samples : \", len(row_documents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of samples :  1997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOoWStpTGqf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a8e6cf74-2617-4c41-8ee6-538523fcaa44"
      },
      "source": [
        "print('converting text data to csv file..')\n",
        "claim = []\n",
        "body = []\n",
        "question = []\n",
        "part = []\n",
        "headline = []\n",
        "label = []    \n",
        "index = []\n",
        "i = 0\n",
        "row_doc = np.asarray(row_documents)\n",
        "for row in row_doc:\n",
        "    claim.append(row.split(',')[0])\n",
        "    body.append(row.split(',')[1])\n",
        "    question.append(row.split(',')[2])\n",
        "    part.append(row.split(',')[3])\n",
        "    headline.append(row.split(',')[4])\n",
        "    label.append(row.split(',')[-1])\n",
        "    index.append(i)\n",
        "    i += 1\n",
        "    \n",
        "Dataset = list(zip(index, claim, body, question, part, headline, label))\n",
        "print(Dataset[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "converting text data to csv file..\n",
            "(0, 'مسمومیت زا بودن شدید قارچهای خودرو (وحشی) مشابه قارچهای موجود در بازار !', 'به گزارش اقتصادآنلاین به نقل از تسنیم، رخداد مسمومیت قارچی بهار 97؛ حادثه\\u200cهای مرگباری بود که نه از طریق تصادف یا سقوط و یا غرق شدگی - برای مسافران و گردشگران رخ داده باشد؛ بلکه در رویدادی کم تکرار - علت این مرگ\\u200cها قارچ بوده است! بطوریکه تا  غروب دوشنبه 31 اردیبهشت 1017 نفر به بیمارستان\\u200cها مراجعه کردند که از این تعداد 116 نفر در بیمارستان بستری شدند و متأسفانه 15 نفر جان خود را از دست دادند.\\n\\nدر این زمینه برای واکاوی بیشتر این رویداد تلخ و تاسف بار که می\\u200cطلبید برخی نهادهای مسؤل در حوزه طبیعت از جمله سازمان محیط زیست و .... قبل از وقوع این حوادث که به زعم کارشناسان امر یکی از دلایل مهم آن بارندگی زیاد و رویش این قارچ\\u200cها بود، مردم نسبت به استفاده نکردن خوراکی این قارچ\\u200cها حداقل از نظر آگاهی رسانی مطلع و به آنان هشدار داده می\\u200cشد.', '0', '0', 'Discuss', 'Discuss')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZo7W-BrHPQB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "536785db-7f7c-449f-d34c-26ed0001ef21"
      },
      "source": [
        "df = pd.DataFrame(data = Dataset, columns=['index', 'claim', 'body', 'question', 'part', 'headline', 'label'])\n",
        "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print('done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTYhVbW5ovEh"
      },
      "source": [
        "dataset = pd.read_csv(GDRIVE_DIR / 'persian_stance_deep_data' / 'dataset' / 'ArticleToClaim.csv', index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25NC72fy3VV5"
      },
      "source": [
        "dataset = dataset.sort_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWGYroc8aFa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d456cdc6-8c3f-44ca-c5d2-ef250860dc3b"
      },
      "source": [
        "dataset.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['claim', 'body', 'question', 'part', 'headline', 'label'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLIiViZiabv8"
      },
      "source": [
        "claim = dataset['claim']\n",
        "headline = dataset['headline']\n",
        "body = dataset['body']\n",
        "label = dataset['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT_sz_7fbYq-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "75ed1588-7b44-4688-91f5-537695dcad19"
      },
      "source": [
        "len(body)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV2OY21RatRc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "20a79659-0d87-4491-e958-62c13a01d08d"
      },
      "source": [
        "print(claim[0], \"\\n\", body[0], \"\\n\", label[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "مسمومیت زا بودن شدید قارچهای خودرو (وحشی) مشابه قارچهای موجود در بازار ! \n",
            " به گزارش اقتصادآنلاین به نقل از تسنیم، رخداد مسمومیت قارچی بهار 97؛ حادثه‌های مرگباری بود که نه از طریق تصادف یا سقوط و یا غرق شدگی - برای مسافران و گردشگران رخ داده باشد؛ بلکه در رویدادی کم تکرار - علت این مرگ‌ها قارچ بوده است! بطوریکه تا  غروب دوشنبه 31 اردیبهشت 1017 نفر به بیمارستان‌ها مراجعه کردند که از این تعداد 116 نفر در بیمارستان بستری شدند و متأسفانه 15 نفر جان خود را از دست دادند.\n",
            "\n",
            "در این زمینه برای واکاوی بیشتر این رویداد تلخ و تاسف بار که می‌طلبید برخی نهادهای مسؤل در حوزه طبیعت از جمله سازمان محیط زیست و .... قبل از وقوع این حوادث که به زعم کارشناسان امر یکی از دلایل مهم آن بارندگی زیاد و رویش این قارچ‌ها بود، مردم نسبت به استفاده نکردن خوراکی این قارچ‌ها حداقل از نظر آگاهی رسانی مطلع و به آنان هشدار داده می‌شد. \n",
            " Discuss\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8S0PS0VzZ1X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "2124bf89-3000-428b-8808-47b774f95c2a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(4,3))\n",
        "dataset.groupby('label').headline.count().plot.bar(ylim=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo637O3-zaMi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "8dd68d92-81e1-462c-e953-20f3d4775fb3"
      },
      "source": [
        "dataset.groupby('label').headline.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "Agree         137\n",
              "Disagree      206\n",
              "Discuss      1068\n",
              "Unrelated     586\n",
              "Name: headline, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVFvfaRAnGeK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "32fbe099-3343-4b88-d87f-5a0d0d829fcc"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ZJraSam5rG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "23095370-8f30-444a-9da7-a8514b35c228"
      },
      "source": [
        "dataset.groupby('label').headline.count() * 100/len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "Agree         6.860290\n",
              "Disagree     10.315473\n",
              "Discuss      53.480220\n",
              "Unrelated    29.344016\n",
              "Name: headline, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibRE0Lj9tSMa"
      },
      "source": [
        "# Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLuPx6X7tV8X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "5310d271-5439-415f-dfd5-1c562f0eb98a"
      },
      "source": [
        "!pip install stanfordnlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanfordnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/bf/5d2898febb6e993fcccd90484cba3c46353658511a41430012e901824e94/stanfordnlp-0.2.0-py3-none-any.whl (158kB)\n",
            "\r\u001b[K     |██                              | 10kB 15.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20kB 1.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30kB 1.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 51kB 1.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 61kB 1.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 71kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 81kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 92kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 102kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 112kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 122kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 133kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 143kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 153kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.18.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.38.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (46.1.3)\n",
            "Installing collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUA9OlQctvSR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "8c2da7d4-3ae2-4880-9080-61797d17914a"
      },
      "source": [
        "import stanfordnlp\n",
        "stanfordnlp.download('fa')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"fa_seraji\" for language \"fa\".\n",
            "Would you like to download the models for: fa_seraji now? (Y/n)\n",
            "y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: fa_seraji\n",
            "Download location: /root/stanfordnlp_resources/fa_seraji_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 226M/226M [00:15<00:00, 14.2MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/fa_seraji_models.zip\n",
            "Extracting models file for: fa_seraji\n",
            "Cleaning up...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygMCI-pft5qW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "f6855b20-3270-407a-c88a-b46af056f5c7"
      },
      "source": [
        "import stanfordnlp\n",
        "nlp = stanfordnlp.Pipeline(processors='tokenize,lemma', lang='fa', treebank=None, use_gpu=True) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji_tokenizer.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/fa_seraji_models/fa_seraji_lemmatizer.pt', 'lang': 'fa', 'shorthand': 'fa_seraji', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "Done loading processors!\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMU7fE5c1bIh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "9be2b1da-e029-4c18-e0ad-f31161bf22d8"
      },
      "source": [
        "!pip install hazm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 44.9MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.12.0)\n",
            "Building wheels for collected packages: libwapiti, nltk\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp36-cp36m-linux_x86_64.whl size=153754 sha256=c42133518ed0a65e27331446f99b7cac6479acad7921ebb3f1614d9cfd22d14c\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394471 sha256=ac1d6d84cf39eab6bfa238caa8e605fbcfa47101c992344c1369db27ce48fb87\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built libwapiti nltk\n",
            "Installing collected packages: libwapiti, nltk, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdy5FjpMvUqZ"
      },
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "\n",
        "  \n",
        "k = []\n",
        "with open(os.path.join(GDRIVE_DIR, \"persian_stance_baseline_data/dataset/stopWords.txt\"), 'r', encoding=\"utf-8\") as f:\n",
        "    for word in f:\n",
        "        word = word.split('\\n')\n",
        "        k.append(word[0])\n",
        "        \n",
        "def remove_stopwords(text):\n",
        "    sw_data = []\n",
        "    for i in text:\n",
        "        for j in k:\n",
        "            if j in word_tokenize(i):\n",
        "                i.replace(j, '')\n",
        "        sw_data.append(i)\n",
        "    return sw_data\n",
        "\n",
        "\n",
        "def remove_slash(text):\n",
        "    ext_data = []\n",
        "    for i in text:\n",
        "        if '/' in i:\n",
        "            spl = i.split('/')\n",
        "            if 'شایعه' in spl[-1]:\n",
        "                i = i.replace(spl[-1], '')\n",
        "        ext_data.append(i)\n",
        "    return ext_data\n",
        "\n",
        "\n",
        "import re\n",
        "r = re.compile(\"[\\!\\;,؟:?،؛.+»«<>|\\#(\\)\\-\\/\\'\\\"]\")\n",
        "def remove_punc(text):\n",
        "    punc = []\n",
        "    for i in text:\n",
        "        punc.append(r.sub(\"\", i))\n",
        "    return punc\n",
        "\n",
        "extra_str = ['\\u200c', '\\u200d', '\\u200e', '\\u200b', '\\r', '\\n', '\\ufeff']\n",
        "def clean_data(text):\n",
        "    \n",
        "    print(\"start cleaning data..\")\n",
        "    \n",
        "    text = remove_slash(text)\n",
        "    \n",
        "    clean_data = []\n",
        "    for i in text:\n",
        "        for j in extra_str:\n",
        "            if j in i:\n",
        "                i = i.replace(j,'')\n",
        "        clean_data.append(i)\n",
        "    \n",
        "    print(\"data is ready!\")\n",
        "    return clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZk9XZU7Q8U3"
      },
      "source": [
        "import re\n",
        "r = re.compile(\"[\\!\\;,؟:?،؛.+»«<>|\\#(\\)\\-\\/\\'\\\"]\")\n",
        "def clean(text):\n",
        "    return r.sub(\"\", text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOeLIjB_wfa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b6c9049b-a783-4107-89ee-109ce55b1fa1"
      },
      "source": [
        "clean_claim = clean_data(claim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start cleaning data..\n",
            "data is ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKhhrM922IbE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0a1e34d1-cd9e-4629-f6bc-93b2bd153fc9"
      },
      "source": [
        "clean_body = clean_data(body)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start cleaning data..\n",
            "data is ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M69ZHTh538Sb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "311a7a21-34e5-43d3-a3b3-1ddaf64afa37"
      },
      "source": [
        "Dataset = list(zip(clean_claim, clean_body, label))\n",
        "np.random.shuffle(Dataset)  \n",
        "df = pd.DataFrame(data = Dataset, columns=['claim', 'body', 'label'])\n",
        "df.to_csv(GDRIVE_DIR / 'persian_stance_deep_data' / 'dataset' / 'clean_claim_body.csv', index=True, encoding=\"utf-8\")\n",
        "\n",
        "print('save clean data.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "save clean data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa_zU2y2vD_K"
      },
      "source": [
        "import pandas as pd\n",
        "dataset_clean = pd.read_csv(GDRIVE_DIR / 'persian_stance_deep_data' / 'dataset' / 'clean_claim_body.csv', index_col = 0, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUhHHXho1a9n"
      },
      "source": [
        "clean_claim = dataset_clean['claim']\n",
        "clean_body = dataset_clean['body']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJUbSCMfvUqC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "9688de85-8339-45f5-8b46-3d2d9e388f70"
      },
      "source": [
        "dataset_clean.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim</th>\n",
              "      <th>body</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>کشف موجود عجیبی شبیه انسان در یک حفاری در پاکس...</td>\n",
              "      <td>در حادثه ای عجیب و غریب و تکان دهنده در جادهاپ...</td>\n",
              "      <td>Unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>خودروی داعش در زنجان ! /</td>\n",
              "      <td>دانا: چند روزی است که خبری مبنی بر اینکه یک خو...</td>\n",
              "      <td>Discuss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>اجساد نظامیان کشته شده ایرانی در حملات اخیر اس...</td>\n",
              "      <td>به گزارش این رسانهها حجتالله نوچمنی از شهر کرد...</td>\n",
              "      <td>Discuss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>خوردن گوشت گربه در مناطقی از ایران؛ برای نجات ...</td>\n",
              "      <td>ه گزارش جماران؛ حسن شمشادی خبرنگار صدا و سیما ...</td>\n",
              "      <td>Discuss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>تصاویر مشروب های تولید کارخانجات ایرانی، برای ...</td>\n",
              "      <td>به گزارش افکارنیوز، روز گذشته عکسی از یک بطری ...</td>\n",
              "      <td>Disagree</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               claim  ...      label\n",
              "0  کشف موجود عجیبی شبیه انسان در یک حفاری در پاکس...  ...  Unrelated\n",
              "1                           خودروی داعش در زنجان ! /  ...    Discuss\n",
              "2  اجساد نظامیان کشته شده ایرانی در حملات اخیر اس...  ...    Discuss\n",
              "3  خوردن گوشت گربه در مناطقی از ایران؛ برای نجات ...  ...    Discuss\n",
              "4  تصاویر مشروب های تولید کارخانجات ایرانی، برای ...  ...   Disagree\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL2Q4zNQvpT5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = dataset_clean.label\n",
        "X = dataset_clean.drop('label', axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_ur1L11xOJL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4d221a73-97e3-414f-cb01-955b20da4eec"
      },
      "source": [
        "Data_train = list(zip(X_train['claim'], X_train['body'], y_train))\n",
        "    \n",
        "df = pd.DataFrame(data = Data_train, columns=['claim', 'body', 'label'])\n",
        "df.to_csv(GDRIVE_DIR / 'persian_stance_deep_data' / 'dataset' / 'train_data.csv', index=True, encoding=\"utf-8\")\n",
        "\n",
        "print('save train data.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "save train data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atY7I1cxy_6C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a8bda498-4306-424b-aac8-21bcbe694092"
      },
      "source": [
        "Data_test = list(zip(X_test['claim'], X_test['body'], y_test))\n",
        "    \n",
        "df = pd.DataFrame(data = Data_test, columns=['claim', 'body', 'label'])\n",
        "df.to_csv(GDRIVE_DIR /'persian_stance_deep_data' / 'dataset' / 'test_data.csv', index=True, encoding=\"utf-8\")\n",
        "\n",
        "print('save test data.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "save test data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvvkjOz0ynTI"
      },
      "source": [
        "data_train = pd.read_csv(GDRIVE_DIR / 'persian_stance_deep_data' / 'dataset' / 'train_data.csv', index_col = 0, )\n",
        "data_test = pd.read_csv(GDRIVE_DIR / 'persian_stance_deep_data' / 'dataset' / 'test_data.csv', index_col = 0, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThVeCTtnysw-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f9952cd2-f96d-44d1-a067-e71de43842be"
      },
      "source": [
        "len(data_train), len(data_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1597, 400)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5kmac9sZtlM"
      },
      "source": [
        "# Extract Feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVM3qiwYiytW"
      },
      "source": [
        "## bow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBbHZ8W6bND6"
      },
      "source": [
        "def ngrams(input, n):\n",
        "    input = input.split(' ')\n",
        "    output = []\n",
        "    for i in range(len(input) - n + 1):\n",
        "        output.append(input[i:i + n])\n",
        "    return output\n",
        "\n",
        "\n",
        "def chargrams(input, n):\n",
        "    output = []\n",
        "    for i in range(len(input) - n + 1):\n",
        "        output.append(input[i:i + n])\n",
        "    return output\n",
        "\n",
        "\n",
        "def append_chargrams(features, text_headline, text_body, size):\n",
        "    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(text_headline.split())), size)]\n",
        "    grams_hits = 0\n",
        "    grams_early_hits = 0\n",
        "    grams_first_hits = 0\n",
        "    for gram in grams:\n",
        "        if gram in text_body:\n",
        "            grams_hits += 1\n",
        "        if gram in text_body[:255]:\n",
        "            grams_early_hits += 1\n",
        "        if gram in text_body[:100]:\n",
        "            grams_first_hits += 1\n",
        "    features.append(grams_hits)\n",
        "    features.append(grams_early_hits)\n",
        "    features.append(grams_first_hits)\n",
        "    return features\n",
        "\n",
        "\n",
        "def append_ngrams(features, text_headline, text_body, size):\n",
        "    grams = [' '.join(x) for x in ngrams(text_headline, size)]\n",
        "    grams_hits = 0\n",
        "    grams_early_hits = 0\n",
        "    for gram in grams:\n",
        "        if gram in text_body:\n",
        "            grams_hits += 1\n",
        "        if gram in text_body[:255]:\n",
        "            grams_early_hits += 1\n",
        "    features.append(grams_hits)\n",
        "    features.append(grams_early_hits)\n",
        "    return features\n",
        "\n",
        "\n",
        "def hand_features(headlines, bodies):\n",
        "\n",
        "    def binary_co_occurence(headline, body):\n",
        "        # Count how many times a token in the title\n",
        "        # appears in the body text.\n",
        "        bin_count = 0\n",
        "        bin_count_early = 0\n",
        "        for headline_token in clean(headline).split(\" \"):\n",
        "            if headline_token in clean(body):\n",
        "                bin_count += 1\n",
        "            if headline_token in clean(body)[:255]:\n",
        "                bin_count_early += 1\n",
        "        return [bin_count, bin_count_early]\n",
        "\n",
        "    def binary_co_occurence_stops(headline, body):\n",
        "        # Count how many times a token in the title\n",
        "        # appears in the body text. Stopwords in the title\n",
        "        # are ignored.\n",
        "        bin_count = 0\n",
        "        bin_count_early = 0\n",
        "        for headline_token in remove_stopwords(clean(headline).split(\" \")):\n",
        "            if headline_token in clean(body):\n",
        "                bin_count += 1\n",
        "                bin_count_early += 1\n",
        "        return [bin_count, bin_count_early]\n",
        "\n",
        "    def count_grams(headline, body):\n",
        "        # Count how many times an n-gram of the title\n",
        "        # appears in the entire body, and intro paragraph\n",
        "\n",
        "        clean_body = clean(body)\n",
        "        clean_headline = clean(headline)\n",
        "        features = []\n",
        "        features = append_chargrams(features, clean_headline, clean_body, 2)\n",
        "        features = append_chargrams(features, clean_headline, clean_body, 8)\n",
        "        features = append_chargrams(features, clean_headline, clean_body, 4)\n",
        "        features = append_chargrams(features, clean_headline, clean_body, 16)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 2)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 3)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 4)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 5)\n",
        "        features = append_ngrams(features, clean_headline, clean_body, 6)\n",
        "        return features\n",
        "\n",
        "    X = []\n",
        "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
        "        X.append(binary_co_occurence(headline, body)\n",
        "                 + binary_co_occurence_stops(headline, body)\n",
        "                 + count_grams(headline, body))\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZK6HZjTUMdM"
      },
      "source": [
        "# x = hand_features(clean_claim, clean_body)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UetOVrqMzoR8"
      },
      "source": [
        "def get_head_body_tuples(include_holdout=False):\n",
        "    # file paths\n",
        "    '''\n",
        "    data_path = \"%s/data/fnc-1\" % (path.dirname(path.dirname(path.dirname(path.abspath(__file__)))))\n",
        "    splits_dir = \"%s/data/fnc-1/splits\" % (path.dirname(path.dirname(path.dirname(path.abspath(__file__)))))\n",
        "    dataset = DataSet(data_path)\n",
        "    '''\n",
        "    data_path = myConstants.data_path\n",
        "    splits_dir = myConstants.splits_dir\n",
        "    dataset = myConstants.d\n",
        "\n",
        "    def get_stances(dataset, folds, holdout):\n",
        "        # Creates the list with a dict {'headline': ..., 'body': ..., 'stance': ...} for each\n",
        "        # stance in the data set (except for holdout)\n",
        "        stances = []\n",
        "        for stance in dataset.stances:\n",
        "            if stance['Body ID'] in holdout and include_holdout == True:\n",
        "                stances.append(stance)\n",
        "            for fold in folds:\n",
        "                if stance['Body ID'] in fold:\n",
        "                    stances.append(stance)\n",
        "\n",
        "        return stances\n",
        "\n",
        "    # create new vocabulary\n",
        "    folds, holdout = kfold_split(data_train, n_folds=10, base_dir=splits_dir)  # [[133,1334,65645,], [32323,...]] => body ids for each fold\n",
        "    stances = get_stances(dataset, folds, holdout)\n",
        "\n",
        "    print(\"Stances length: \" + str(len(stances)))\n",
        "\n",
        "    h = []\n",
        "    b = []\n",
        "    # create the final lists with all the headlines and bodies of the set except for holdout\n",
        "    for stance in stances:\n",
        "        h.append(stance['Headline'])\n",
        "        b.append(dataset.articles[stance['Body ID']])\n",
        "\n",
        "    return h, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXmoLmIlb6Z6"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def get_head_body_tuples_test():\n",
        "    d = myConstants.testdataset\n",
        "\n",
        "    h = []\n",
        "    b = []\n",
        "    for stance in d.stances:\n",
        "        h.append(stance['Headline'])\n",
        "        b.append(d.articles[int(stance['Body ID'])])\n",
        "\n",
        "    return h, b\n",
        "\n",
        "# this function takes a very long time to finish execution!\n",
        "def negated_context_word_12grams_concat_tf5000_l2_all_data(headlines, bodies):\n",
        "    \"\"\"\n",
        "    Negates string after special negation word by adding a \"NEG_\" in front\n",
        "    of every negated word, until a punctuation mark appears.\n",
        "    Source:\n",
        "        NRC-Canada: Buidling the State-of-the-Art in Sentiment Analysis of Tweets\n",
        "        http://sentiment.christopherpotts.net/lingstruc.html\n",
        "        http://stackoverflow.com/questions/23384351/how-to-add-tags-to-negated-words-in-strings-that-follow-not-no-and-never\n",
        "\n",
        "\n",
        "    :param headlines:\n",
        "    :param bodies:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    def get_negated_text(text):\n",
        "      sens = text.replace(';','.').replace(',','.').replace('!','.').replace(':','.').replace('،', '').split('.')\n",
        "      li_1 = ['هیچ', 'اصلا', 'هیچگونه']\n",
        "      li_2 = [ 'ندارد', 'نمیتواند']\n",
        "      jomles = []\n",
        "      for sen in sens: \n",
        "        first, second = 0 , 0\n",
        "        flag_1, flag_2 = False, False\n",
        "        tokens = word_tokenize(sen)\n",
        "        jomle = []    \n",
        "        for i in range(len(tokens)):\n",
        "          if tokens[i] in li_1 and flag_1 == False:\n",
        "            first = i\n",
        "            flag_1 = True\n",
        "          if tokens[i] in li_2 and flag_2 == False:\n",
        "            second = i\n",
        "            flag_2 = True\n",
        "        if (second > first) and (flag_1 == True) and (flag_2 == True):\n",
        "          for j in range (first + 1 , second-1 ):\n",
        "            sen = sen.replace(tokens[j], 'NEG_'+tokens[j])\n",
        "        jomles.append(sen)\n",
        "    \n",
        "      jomles = '. '.join(jomles)\n",
        "  \n",
        "      return jomles\n",
        "\n",
        "    def combine_head_and_body(headlines, bodies):\n",
        "        head_and_body = [headline + \" \" + body for i, (headline, body) in\n",
        "                         enumerate(zip(headlines, bodies))]\n",
        "\n",
        "        return head_and_body\n",
        "\n",
        "    def get_vocab(neg_headlines, neg_bodies):\n",
        "        neg_headlines = remove_stopwords(neg_headlines)\n",
        "        neg_bodies = remove_stopwords(neg_bodies)\n",
        "        tf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, use_idf=False,\n",
        "                                        norm='l2')\n",
        "        tf_vectorizer.fit_transform(combine_head_and_body(neg_headlines, neg_bodies))\n",
        "        vocab = tf_vectorizer.vocabulary_\n",
        "\n",
        "        return vocab\n",
        "\n",
        "    def get_features(neg_headlines_test, neg_bodies_test, vocab):\n",
        "        neg_headlines_test = remove_stopwords(neg_headlines_test)\n",
        "        neg_bodies_test = remove_stopwords(neg_bodies_test)\n",
        "        \n",
        "        tf_vectorizer_head = TfidfVectorizer(vocabulary=vocab, use_idf=False, norm='l2')\n",
        "        X_test_head = tf_vectorizer_head.fit_transform(neg_headlines_test)\n",
        "\n",
        "        tf_vectorizer_body = TfidfVectorizer(vocabulary=vocab, use_idf=False, norm='l2')\n",
        "        X_test_body = tf_vectorizer_body.fit_transform(neg_bodies_test)\n",
        "\n",
        "        X_test = np.concatenate([X_test_head.toarray(), X_test_body.toarray()], axis=1)\n",
        "        return X_test\n",
        "\n",
        "#     h, b = get_head_body_tuples(include_holdout=True)\n",
        "#     h_test, b_test = get_head_body_tuples_test()\n",
        "    \n",
        "    h, b = data_train['claim'].tolist() , data_train['body'].tolist()\n",
        "    h_test, b_test = data_test['claim'].tolist(), data_test['body'].tolist()\n",
        "\n",
        "    # Comment out for clean ablation tests\n",
        "    h.extend(h_test)\n",
        "    b.extend(b_test)\n",
        "\n",
        "    neg_headlines_all = [get_negated_text(h) for h in h]\n",
        "    neg_bodies_all = [get_negated_text(b) for b in b]\n",
        "    neg_headlines = [get_negated_text(h) for h in headlines]\n",
        "    neg_bodies = [get_negated_text(b) for b in bodies]\n",
        "\n",
        "    vocab = get_vocab(neg_headlines_all, neg_bodies_all)\n",
        "    X_train = get_features(neg_headlines, neg_bodies, vocab)\n",
        "\n",
        "    return X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KlrvAVTrQ-R"
      },
      "source": [
        "# x = negated_context_word_12grams_concat_tf5000_l2_all_data(clean_claim, clean_body)\n",
        "# this line is commented in the main notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqAM-p6QcGXe"
      },
      "source": [
        "def char_3grams_5000_concat_all_data(headlines, bodies):\n",
        "\n",
        "    def combine_head_and_body(headlines, bodies):\n",
        "        return [headline + \" \" + body for i, (headline, body) in\n",
        "                tqdm(enumerate(zip(headlines, bodies)))]\n",
        "\n",
        "    # Load train data into CountVectorizer, get the resulting X-values and also the vocabulary\n",
        "    # for the test data feature creation\n",
        "    def get_features(headlines, bodies, headlines_all, bodies_all):\n",
        "        # create vocab on basis of training data\n",
        "        head_and_body = combine_head_and_body(headlines_all, bodies_all)\n",
        "        head_and_body_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), lowercase=True,\n",
        "                                              max_features=5000, use_idf=False, norm='l2')\n",
        "        head_and_body_tfidf.fit(head_and_body)\n",
        "        vocab = head_and_body_tfidf.vocabulary_\n",
        "\n",
        "        # create training feature vectors\n",
        "        X_train_head_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), lowercase=True,\n",
        "                                             stop_words='english', vocabulary=vocab, use_idf=False, norm='l2')\n",
        "        X_train_head = X_train_head_tfidf.fit_transform(headlines)\n",
        "\n",
        "        X_train_body_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), lowercase=True,\n",
        "                                             stop_words='english', vocabulary=vocab, use_idf=False, norm='l2')\n",
        "        X_train_body = X_train_body_tfidf.fit_transform(bodies)\n",
        "\n",
        "        X_train = np.concatenate([X_train_head.toarray(), X_train_body.toarray()], axis=1)\n",
        "\n",
        "        return X_train\n",
        "\n",
        "    h, b = data_train['claim'].tolist() , data_train['body'].tolist()\n",
        "    h_test, b_test = data_test['claim'].tolist(), data_test['body'].tolist()\n",
        "\n",
        "    # Comment out for clean ablation tests\n",
        "    h.extend(h_test)\n",
        "    b.extend(b_test)\n",
        "\n",
        "    X_train = get_features(headlines, bodies, h, b)\n",
        "\n",
        "    return X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb5fB-Eg5pB6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "55337d59-aac4-494d-a77a-0991fb98c020"
      },
      "source": [
        "x = char_3grams_5000_concat_all_data(clean_claim, clean_body)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1997it [00:00, 228909.43it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bkNyP0i58mE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6c556120-c6e3-4567-ee45-99f84194c910"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1997, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPN7xgRFi2V-"
      },
      "source": [
        "## embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q8n3pQ-o4Wh"
      },
      "source": [
        "### word embedding persian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6K7iuEgJIhT"
      },
      "source": [
        "import gzip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BQ1bFu7UBdk"
      },
      "source": [
        "def load_embedding_pandas(FILE, type=\"w2v\"):\n",
        "  embeddings_index=dict()\n",
        "  f = open(FILE)\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "  print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "  return embeddings_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXcSLm-SUmKh"
      },
      "source": [
        "GloVe_vectors = load_embedding_pandas(\"/content/drive/My Drive/persian_stance_baseline_data/vectors/cc.fa.300.vec\")\n",
        "# takes a little time...don't worry!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omDwJqfYbRGU"
      },
      "source": [
        "g_vec = pd.DataFrame.from_dict(GloVe_vectors)\n",
        "# takes a little time...don't worry!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5skj76c3apYu"
      },
      "source": [
        "import zipfile\n",
        "import numpy as np\n",
        "import os.path as path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pickle\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "FEATURES_DIR = '/content/drive/My Drive/persian_stance_deep_data/dataset/features/'\n",
        "EMBEDDINGS_DIR = \"/content/drive/My Drive/persian_stance_deep_data/dataset/embedding/\" \n",
        "DATA_PATH = \"/content/drive/My Drive/persian_stance_deep_data/dataset/\"\n",
        "SPLITS_DIR = \"/content/drive/My Drive/persian_stance_deep_data/dataset/splits/\"\n",
        "\n",
        "def create_embedding_lookup_pandas(text_list, max_nb_words, embedding_dim, embedding,\n",
        "                            embedding_lookup_name, embedding_vocab_name, rdm_emb_init=False, add_unknown=False, tokenizer=None, init_zeros = False):\n",
        "    \"\"\"\n",
        "    Creates the claim embedding lookup table if it not already exists and returns the vocabulary for it\n",
        "    :param text_list:\n",
        "    :param max_nb_words:\n",
        "    :param embedding_dim:\n",
        "    :param GloVe_vectors:\n",
        "    :param embedding_lookup_name:\n",
        "    :param embedding_vocab_name:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    #del GloVe_vectors\n",
        "\n",
        "    # if ...embedding.npy or vocab.pkl files don't exist: \n",
        "    if not path.exists(FEATURES_DIR + embedding_lookup_name) or not path.exists(FEATURES_DIR + embedding_vocab_name):\n",
        "        print(\"can't find npy or pkl file!\")\n",
        "        vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=None, tokenizer=tokenizer,\n",
        "                                            max_features=max_nb_words, use_idf=True)\n",
        "        vectorizer.fit_transform(text_list)\n",
        "        vocab = vectorizer.vocabulary_\n",
        "\n",
        "\n",
        "        # do not use 0 since we want to use masking in the LSTM later on\n",
        "        for word in vocab.keys():\n",
        "            vocab[word] += 1\n",
        "        if add_unknown == True:\n",
        "            max_index = max(vocab.values())\n",
        "            vocab[\"UNKNOWN\"] = max_index+1\n",
        "\n",
        "        # prepare embedding - create matrix that holds the glove vector for each vocab entry\n",
        "        if rdm_emb_init == True:\n",
        "            embedding_lookup = np.random.random((len(vocab) + 1, embedding_dim))\n",
        "            zero_vec = np.zeros((embedding_dim))\n",
        "            embedding_lookup[0] = zero_vec # for masking\n",
        "        else:\n",
        "            embedding_lookup = np.zeros((len(vocab) + 1, embedding_dim))\n",
        "\n",
        "        if init_zeros == False:\n",
        "            for word, i in vocab.items():\n",
        "                if word == \"UNKNOWN\":\n",
        "                    embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dim)\n",
        "                    #print(embedding_vector)\n",
        "                else:\n",
        "                    try:\n",
        "                        embedding_vector = embedding.loc[word].as_matrix()\n",
        "                    except KeyError: #https://stackoverflow.com/questions/15653966/ignore-keyerror-and-continue-program\n",
        "                        continue\n",
        "                if embedding_vector is not None:\n",
        "                    # words not found in embedding index will be all-zeros.\n",
        "                    embedding_lookup[i] = embedding_vector\n",
        "        print(\"created embedding lookup!\")\n",
        "        #print(embedding_lookup[-1])\n",
        "        # save embedding matrix\n",
        "        np.save(FEATURES_DIR + embedding_lookup_name, embedding_lookup)\n",
        "        print(\"embedding matrix saved!\")\n",
        "        # save vocab\n",
        "        with open(FEATURES_DIR + embedding_vocab_name, 'wb') as f:\n",
        "            pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n",
        "        print(\"vocab saved!\")\n",
        "\n",
        "        print(\"Embedding lookup table shape for \" + embedding_lookup_name + \" is: \" + str(embedding_lookup.shape))\n",
        "    #if both .npy and .pkl files exist:\n",
        "    else:\n",
        "        print(\"found npy and pkl files!\")\n",
        "        with open(FEATURES_DIR + embedding_vocab_name, \"rb\") as f:\n",
        "            vocab = pickle.load(f)\n",
        "\n",
        "    print(\"Vocab size for \" + embedding_vocab_name + \" is: \" + str(len(vocab)))\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def text_to_sequences_fixed_size(texts, vocab, MAX_SENT_LENGTH, save_full_text=False, take_full_claim = False):\n",
        "    \"\"\"\n",
        "    Turns sentences of claims into sequences of indices provided by the given vocab.\n",
        "    Unknown words will get an extra index, if\n",
        "    the vocab has a token \"UNKNOWN\". The method takes the longest sentence of the claims, if the\n",
        "    claim should have more than one sentence.\n",
        "    :param texts:\n",
        "    :param vocab:\n",
        "    :param MAX_SENT_LENGTH:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    data = np.zeros((len(texts), MAX_SENT_LENGTH), dtype='int32')\n",
        "\n",
        "    claims = []\n",
        "    if take_full_claim == False:\n",
        "        for claim in texts:\n",
        "            claim_sents = nltk.sent_tokenize(claim)\n",
        "            word_count_fct = lambda sentence: len(nltk.word_tokenize(sentence)) # take longest sentence of claim if it has more than one\n",
        "            claims.append(max(claim_sents, key=word_count_fct))\n",
        "    else:\n",
        "        claims = texts\n",
        "\n",
        "    data_string_dict = {}\n",
        "    for j, claim in tqdm(enumerate(claims)):\n",
        "        claim_tokens = nltk.word_tokenize(claim.lower())\n",
        "\n",
        "        data_string = \"\"\n",
        "        if save_full_text == True:\n",
        "            for token in claim_tokens:\n",
        "                data_string += token + \" \"\n",
        "            data_string = data_string[:-1]\n",
        "            data_string_dict[j] = data_string\n",
        "\n",
        "        for i, token in enumerate(claim_tokens):\n",
        "            if i < MAX_SENT_LENGTH:\n",
        "                index = vocab.get(token, \"UNKNOWN\")\n",
        "                if index == \"UNKNOWN\":\n",
        "                    index = vocab.get(index, None)\n",
        "                if index != None:\n",
        "                    data[j, i] = index\n",
        "\n",
        "    if save_full_text == True:\n",
        "        return data, data_string_dict\n",
        "    else:\n",
        "        return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9uMrbpUYE9m"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNEMxYF06BWf"
      },
      "source": [
        "def single_flat_LSTM_50d_100(headlines, bodies, GloVe_vectors):\n",
        "\n",
        "    #########################\n",
        "    # PARAMETER DEFINITIONS #\n",
        "    #########################\n",
        "    method_name = \"single_flat_LSTM_50d_100\"\n",
        "    # location path for features\n",
        "    FEATURES_DIR = '/content/drive/My Drive/persian_stance_deep_data/dataset/features/'\n",
        "    PARAM_DICT_FILENAME = method_name+\"_param_dict.pkl\"\n",
        "\n",
        "    param_dict = {\n",
        "        \"MAX_NB_WORDS\": 50000,  # size of the vocabulary\n",
        "\n",
        "        # sequence lengths\n",
        "        \"MAX_SEQ_LENGTH\": 100, #1000\n",
        "\n",
        "        # embedding specific values\n",
        "        \"EMBEDDING_DIM\": 50,  # dimension of the GloVe embeddings\n",
        "        \"GLOVE_ZIP_FILE\": \"/content/drive/My Drive/persian_stance_baseline_data/vectors/cc.fa.300.vec.gz\",  #*********************\n",
        "        \"GLOVE_FILE\": \"/content/drive/My Drive/persian_stance_baseline_data/vectors/cc.fa.300.vec\",  #*********************\n",
        "\n",
        "        # embedding file names\n",
        "        \"EMBEDDING_FILE\": method_name+\"_embedding.npy\",\n",
        "\n",
        "        # vocab file names\n",
        "        \"VOCAB_FILE\": method_name+\"_vocab.pkl\",\n",
        "    }\n",
        "\n",
        "\n",
        "    ###############################################\n",
        "    # GET VOCABULARY AND PREPARE EMBEDDING MATRIX #\n",
        "    ###############################################\n",
        "\n",
        "    # load GloVe embeddings\n",
        "    # load the whole embedding into memory\n",
        "    \n",
        "#     GloVe_vectors = load_embedding_pandas(param_dict[\"GLOVE_FILE\"])\n",
        "    \n",
        "\n",
        "    # load all claims, orig_docs and evidences\n",
        "    all_heads, all_bodies = data_train['claim'].tolist() , data_train['body'].tolist()\n",
        "    all = all_heads\n",
        "    all.extend(all_bodies)\n",
        "   \n",
        "    \n",
        "\n",
        "    # Comment out for clean ablation checks\n",
        "    # add the unlabeled test data words to the BoW of test+train+holdout data\n",
        "    h_unlbled_test, b_unlbled_test = data_test['claim'].tolist(), data_test['body'].tolist()\n",
        "    all.extend(h_unlbled_test)\n",
        "    all.extend(b_unlbled_test)\n",
        "    \n",
        "\n",
        "    # create and save the embedding matrices for claims, orig_docs and evidences\n",
        "    vocab = create_embedding_lookup_pandas(all, param_dict[\"MAX_NB_WORDS\"], param_dict[\"EMBEDDING_DIM\"],\n",
        "                                           GloVe_vectors, param_dict[\"EMBEDDING_FILE\"], param_dict[\"VOCAB_FILE\"], init_zeros=False,\n",
        "                                           add_unknown=True, rdm_emb_init=True, tokenizer=nltk.word_tokenize)\n",
        "\n",
        "    # unload GloVe_vectors in order to make debugging possible\n",
        "    del GloVe_vectors\n",
        "\n",
        "\n",
        "    #################################################\n",
        "    # Create sequences and embedding for the claims #\n",
        "    #################################################\n",
        "    print(\"Create sequences and embedding for the heads\")\n",
        "\n",
        "    concatenated = []\n",
        "    for i in range(len(headlines)):\n",
        "        concatenated.append(headlines[i] + \". \" + bodies[i])\n",
        "\n",
        "    # replace tokens of claims by vocabulary ids - the ids refer to the index of the embedding matrix which holds the word embedding for this vocab word\n",
        "    sequences = text_to_sequences_fixed_size(concatenated, vocab, param_dict[\"MAX_SEQ_LENGTH\"], save_full_text=False,\n",
        "                                             take_full_claim=True)\n",
        "\n",
        "\n",
        "\n",
        "    #################################################\n",
        "    # SAVE PARAM_DICT AND CONCATENATE TRAINING DATA #\n",
        "    #################################################\n",
        "\n",
        "    # save param_dict\n",
        "    with open(FEATURES_DIR+PARAM_DICT_FILENAME, 'wb') as f:\n",
        "        pickle.dump(param_dict, f, pickle.HIGHEST_PROTOCOL)\n",
        "    print(\"Save PARAM_DICT as \" + FEATURES_DIR+PARAM_DICT_FILENAME)\n",
        "\n",
        "    return sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iYxaWtCXjCF"
      },
      "source": [
        "# aa = single_flat_LSTM_50d_100(docs, docs_2, g_vec) \n",
        "# in the main code(the line above this) docs and docs_2 are not defined so we will try this:\n",
        "aa = single_flat_LSTM_50d_100(clean_claim, clean_body, g_vec)\n",
        "# this is for testing the function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiD6GBFmr-aK"
      },
      "source": [
        "#!gunzip \"/content/drive/My Drive/persian_stance_baseline_data/vectors/cc.fa.300.vec.gz\"\n",
        "# this line is commented in the main notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHFR3Mqyi-EF"
      },
      "source": [
        "## generate feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URLSI-k5h2OH"
      },
      "source": [
        "dataset_clean.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Pz8dQUbElO"
      },
      "source": [
        "bow_feats = ['hand', 'negated_context_word_12grams_concat_tf5000_l2_all_data', 'char_3grams_5000_concat_all_data']\n",
        "\n",
        "word_emb = ['single_flat_LSTM_50d_100']\n",
        "\n",
        "#topic_models = ['latent_dirichlet_allocation_300', 'latent_semantic_indexing_gensim_300_concat', 'NMF_fit_all_concat_300_no_holdout', 'NMF_cos_300']\n",
        "# topic_models = ['latent_dirichlet_allocation_300', 'latent_semantic_indexing_gensim_300_concat_holdout', 'NMF_fit_all_concat_300_and_test', 'NMF_cos_300']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etKS3L3WhF58"
      },
      "source": [
        "def gen_or_load_feats(feat_fn, headlines, bodies, feature_file, feature):\n",
        "    if not os.path.isfile(feature_file):\n",
        "        if 'single_flat_LSTM_50d_100' in feature:\n",
        "            feats = feat_fn(headlines, bodies, g_vec)\n",
        "        else:\n",
        "            feats = feat_fn(headlines, bodies)\n",
        "        np.save(feature_file, feats)\n",
        "\n",
        "    return np.load(feature_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ohJHd6qgCDt"
      },
      "source": [
        "def generate_features(dataset, name, feature_list, features_dir):\n",
        "    \"\"\"\n",
        "    Creates feature vectors out of the provided dataset\n",
        "    \"\"\"\n",
        "    h, b, y = [], [], []\n",
        "\n",
        "    feature_dict = {'hand': hand_features,\n",
        "                    'single_flat_LSTM_50d_100': single_flat_LSTM_50d_100,\n",
        "                    'char_3grams_5000_concat_all_data': char_3grams_5000_concat_all_data,\n",
        "                    'negated_context_word_12grams_concat_tf5000_l2_all_data': negated_context_word_12grams_concat_tf5000_l2_all_data,\n",
        "                    }\n",
        "    \n",
        "    y = dataset['label'].tolist()\n",
        "    h = dataset['claim'].tolist()\n",
        "    b = dataset['body'].tolist()\n",
        "\n",
        "    X_feat = []\n",
        "    feat_list = []\n",
        "    last_index = 0\n",
        "    for feature in feature_list:\n",
        "        print(\"processing \" + feature)\n",
        "        feat = gen_or_load_feats(feature_dict[feature], h, b, features_dir+\"/\"+feature+\".\"+name+'.npy', feature)\n",
        "        feat_list.append((last_index, last_index+len(feat[0]), str(feature)))\n",
        "        last_index += len(feat[0])\n",
        "        X_feat.append(feat)\n",
        "    print(\"done with processing all features in feature_list\")\n",
        "    X = np.concatenate(X_feat, axis=1)\n",
        "\n",
        "    return X, y, feat_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3AzTfruhIhU"
      },
      "source": [
        "features_dir = '/content/drive/My Drive/persian_stance_deep_data/dataset/features'\n",
        "feature_list = word_emb + bow_feats\n",
        "name = 'first_1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHTazG37hJ_r"
      },
      "source": [
        "feature_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyBjbFoBhiMw"
      },
      "source": [
        "X, y , feature_list = generate_features(dataset_clean, name, feature_list, features_dir)\n",
        "# this code takes a very very long time to finish execution if the files required are not generated before...don't worry!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsw4tRMW2Dhh"
      },
      "source": [
        "feature_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r69G5MZ-ZyNs"
      },
      "source": [
        "# Base lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7M8JN4WaYwz"
      },
      "source": [
        "majority vote"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtXP3tKXYlGF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    dataset_clean['claim'], dataset_clean['label'], test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EppwBAJrXEMu"
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "clf_maj = DummyClassifier(strategy=\"most_frequent\")\n",
        "\n",
        "clf_maj.fit(X_train, y_train)\n",
        "\n",
        "y_pred_maj = clf_maj.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgFMeiAsg_Hl"
      },
      "source": [
        "LABELS = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n",
        "LABELS_RELATED = ['unrelated','related']\n",
        "RELATED = LABELS[0:3]\n",
        "\n",
        "def score_submission(gold_labels, test_labels):\n",
        "    score = 0.0\n",
        "    cm = [[0, 0, 0, 0],\n",
        "          [0, 0, 0, 0],\n",
        "          [0, 0, 0, 0],\n",
        "          [0, 0, 0, 0]]\n",
        "\n",
        "    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n",
        "        g_stance, t_stance = g, t\n",
        "        if g_stance == t_stance:\n",
        "            score += 0.25\n",
        "            if g_stance != 'unrelated':\n",
        "                score += 0.50\n",
        "        if g_stance in RELATED and t_stance in RELATED:\n",
        "            score += 0.25\n",
        "\n",
        "        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n",
        "\n",
        "    return score, cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW1IOOYwZKLi"
      },
      "source": [
        "score, cm = score_submission(y_test, y_pred_maj)\n",
        "fold_score, _ = score_submission(y_test, y_pred_maj)\n",
        "max_fold_score, _ = score_submission(y_test, y_test)\n",
        "score = fold_score / max_fold_score\n",
        "\n",
        "print(\"FNC-1 score from restored model: \" +  str(score) +\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylIEsw6ThrYz"
      },
      "source": [
        "def print_confusion_matrix(cm):\n",
        "    lines = []\n",
        "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
        "    line_len = len(header)\n",
        "    lines.append(\"-\"*line_len)\n",
        "    lines.append(header)\n",
        "    lines.append(\"-\"*line_len)\n",
        "\n",
        "    hit = 0\n",
        "    total = 0\n",
        "    for i, row in enumerate(cm):\n",
        "        hit += row[i]\n",
        "        total += sum(row)\n",
        "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n",
        "                                                                   *row))\n",
        "        lines.append(\"-\"*line_len)\n",
        "    print('\\n'.join(lines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIn8-WFuYC3B"
      },
      "source": [
        "print_confusion_matrix(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrJQpoP8Z1xx"
      },
      "source": [
        "# Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm7gN_tDXQpF"
      },
      "source": [
        "def split_X(X_train, MAX_SEQ_LENGTH_HEADS):\n",
        "    # split to get [heads, docs]\n",
        "    X_train_splits = np.hsplit(X_train, np.array([MAX_SEQ_LENGTH_HEADS]))\n",
        "    X_train_head = X_train_splits[0]\n",
        "    X_train_doc = X_train_splits[1]\n",
        "\n",
        "    print(\"X_train_head.shape = \" + str(np.array(X_train_head).shape))\n",
        "    print(\"X_train_doc.shape = \" + str(np.array(X_train_doc).shape))\n",
        "\n",
        "    return X_train_head,X_train_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxDn3uG42KQD"
      },
      "source": [
        "import numpy as np\n",
        "import os.path as path\n",
        "import pickle\n",
        "\n",
        "from keras.layers.core import Dense\n",
        "# from keras.layers.pooling import GlobalMaxPooling1D\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras import optimizers\n",
        "# from fnc.models.Keras_utils import EarlyStoppingOnF1, convert_data_to_one_hot, calculate_class_weight, split_X\n",
        "# from fnc.models.keras_custom_layers.attention_custom import *\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers import Embedding, Input\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV_gZl423WL0"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kphZuuiS7xMe"
      },
      "source": [
        "def convert_data_to_one_hot(y_train):\n",
        "    \n",
        "    y_train_temp = np.zeros((y_train.size, y_train.max() + 1), dtype=np.int)\n",
        "    y_train_temp[np.arange(y_train.size), y_train] = 1\n",
        "\n",
        "    return y_train_temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05-6Ix2L8EkU"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "import keras\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y_train)\n",
        "print(list(le.classes_))\n",
        "y_train = le.transform(y_train)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y_test)\n",
        "print(list(le.classes_))\n",
        "y_test = le.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNNYZbzY5Jjs"
      },
      "source": [
        "def calculate_class_weight(y_train, no_classes=2):\n",
        "    # https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n",
        "    from sklearn.utils import class_weight\n",
        "\n",
        "    class_weight_list = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "    class_weights = {}\n",
        "    for i in range(no_classes):\n",
        "        class_weights[i] = class_weight_list[i]\n",
        "    print(class_weights)\n",
        "    return class_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3K_ngFR2Obp"
      },
      "source": [
        "y_train_one_hot = convert_data_to_one_hot(y_train)\n",
        "y_test_one_hot = convert_data_to_one_hot(y_test)\n",
        "\n",
        "param_dict=\"single_flat_LSTM_50d_100\"\n",
        "\n",
        "FEATURES_DIR = '/content/drive/My Drive/persian_stance_deep_data/dataset/features/'\n",
        "\n",
        "PARAM_DICT_FILENAME = param_dict + \"_param_dict.pkl\"\n",
        "\n",
        "\n",
        "# load feature dict for LSTM_1000_GloVe\n",
        "with open(FEATURES_DIR+PARAM_DICT_FILENAME, \"rb\") as f:\n",
        "  param_dict = pickle.load(f)\n",
        "\n",
        "# load parameters needed for embedding layer\n",
        "EMBEDDING_DIM = param_dict[\"EMBEDDING_DIM\"] # e.g. 50\n",
        "MAX_SEQ_LENGTH = param_dict[\"MAX_SEQ_LENGTH\"] # e.g. 100\n",
        "\n",
        "X_train_LSTM, X_train_MLP = split_X(X_train, MAX_SEQ_LENGTH)\n",
        "X_test_LSTM, X_test_MLP = split_X(X_test, MAX_SEQ_LENGTH)\n",
        "\n",
        "# load embeddings\n",
        "EMBEDDING_FILE = np.load(FEATURES_DIR+param_dict[\"EMBEDDING_FILE\"])\n",
        "\n",
        "print(\"EMBEDDING_FILE.shape = \" + str(EMBEDDING_FILE.shape))\n",
        "\n",
        "# calc cass weights\n",
        "class_weights = calculate_class_weight(y_train, no_classes=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2ZdZpGm5TRs"
      },
      "source": [
        "kernel_initializer = 'glorot_uniform'\n",
        "regularizer = None\n",
        "batch_size=200\n",
        "dense_activity_regularizer=None\n",
        "LSTM_implementation = 2\n",
        "\n",
        "\n",
        "################\n",
        "# CLAIMS LAYER #\n",
        "################\n",
        "lstm_input = Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name='lstm_input') # receive sequences of MAX_SEQ_LENGTH_CLAIMS integers\n",
        "embedding = Embedding(input_dim=len(EMBEDDING_FILE), # lookup table size\n",
        "                                    output_dim=EMBEDDING_DIM, # output dim for each number in a sequence\n",
        "                                    weights=[EMBEDDING_FILE],\n",
        "                                    input_length=MAX_SEQ_LENGTH, # receive sequences of MAX_SEQ_LENGTH_CLAIMS integers\n",
        "                                    mask_zero=True,\n",
        "                                    trainable=True)(lstm_input)\n",
        "data_LSTM = LSTM(\n",
        "            100, return_sequences=True, stateful=False, dropout=0.2,\n",
        "            batch_input_shape=(batch_size, MAX_SEQ_LENGTH, EMBEDDING_DIM),\n",
        "            input_shape=(MAX_SEQ_LENGTH, EMBEDDING_DIM), implementation=LSTM_implementation\n",
        "            )(embedding)\n",
        "data_LSTM = LSTM(\n",
        "            100, return_sequences=False, stateful=False, dropout=0.2,\n",
        "            batch_input_shape=(batch_size, MAX_SEQ_LENGTH, EMBEDDING_DIM),\n",
        "            input_shape=(MAX_SEQ_LENGTH, EMBEDDING_DIM), implementation=LSTM_implementation\n",
        "            )(data_LSTM)\n",
        "\n",
        "###############################\n",
        "# MLP (NON-TIMESTEP) FEATURES #\n",
        "###############################\n",
        "mlp_input = Input(shape=(len(X_train_MLP[0]),), dtype='float32', name='mlp_input')\n",
        "\n",
        "###############\n",
        "# MERGE LAYER #\n",
        "###############\n",
        "merged = concatenate([data_LSTM, mlp_input])\n",
        "\n",
        "dense_mid = Dense(600, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n",
        "                          activity_regularizer=dense_activity_regularizer, activation='relu')(merged)\n",
        "dense_mid = Dense(600, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n",
        "                          activity_regularizer=dense_activity_regularizer, activation='relu')(dense_mid)\n",
        "dense_mid = Dense(600, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n",
        "                          activity_regularizer=dense_activity_regularizer, activation='relu')(dense_mid)\n",
        "dense_out = Dense(4,activation='softmax', name='dense_out')(dense_mid)\n",
        "\n",
        "# build model\n",
        "model = Model(inputs=[lstm_input, mlp_input], outputs=[dense_out])\n",
        "\n",
        "# print summary\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVH4fd1KDJzm"
      },
      "source": [
        "LABELS = ['Agree', 'Disagree', 'Discuss', 'Unrelated']\n",
        "LABELS_RELATED = ['unrelated','related']\n",
        "RELATED = LABELS[0:3]\n",
        "\n",
        "def score_submission(gold_labels, test_labels):\n",
        "    score = 0.0\n",
        "    cm = [[0, 0, 0, 0],\n",
        "          [0, 0, 0, 0],\n",
        "          [0, 0, 0, 0],\n",
        "          [0, 0, 0, 0]]\n",
        "\n",
        "    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n",
        "        g_stance, t_stance = g, t\n",
        "        if g_stance == t_stance:\n",
        "            score += 0.25\n",
        "            if g_stance != 'unrelated':\n",
        "                score += 0.50\n",
        "        if g_stance in RELATED and t_stance in RELATED:\n",
        "            score += 0.25\n",
        "\n",
        "        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n",
        "\n",
        "    return score, cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzm0UREZC3fN"
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "import numpy as np\n",
        "class EarlyStoppingOnF1(Callback):\n",
        "    \"\"\"\n",
        "    Prints some metrics after each epoch in order to observe overfitting\n",
        "                https://github.com/fchollet/keras/issues/5794\n",
        "                custom metrics: https://github.com/fchollet/keras/issues/2607\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epochs,\n",
        "                 X_test_claims,\n",
        "                 X_test_orig_docs,\n",
        "                 y_test, loss_filename, epsilon=0.0, min_epoch = 15, X_test_nt=None):\n",
        "        self.epochs = epochs\n",
        "        self.patience = 2\n",
        "        self.counter = 0\n",
        "        self.prev_score = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.loss_filename = loss_filename\n",
        "        self.min_epoch = min_epoch\n",
        "        self.X_test_nt = X_test_nt\n",
        "        #self.print_train_f1 = print_train_f1\n",
        "\n",
        "        #self.X_train_claims = X_train_claims\n",
        "        #self.X_train_orig_docs = X_train_orig_docs\n",
        "        #self.X_train_evid = X_train_evid\n",
        "        #self.y_train = y_train\n",
        "\n",
        "        self.X_test_claims = X_test_claims\n",
        "        self.X_test_orig_docs = X_test_orig_docs\n",
        "        self.y_test = y_test\n",
        "        Callback.__init__(self)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch + 1 < self.epochs:\n",
        "            from sklearn.metrics import f1_score\n",
        "\n",
        "            # get prediction and convert into list\n",
        "            if type(self.X_test_orig_docs).__module__ == np.__name__ and type(self.X_test_nt).__module__ == np.__name__:\n",
        "                predicted_one_hot = self.model.predict([\n",
        "                    self.X_test_claims,\n",
        "                    self.X_test_orig_docs,\n",
        "                    self.X_test_nt\n",
        "                ])\n",
        "            elif type(self.X_test_orig_docs).__module__ == np.__name__:\n",
        "                predicted_one_hot = self.model.predict([\n",
        "                    self.X_test_claims,\n",
        "                    self.X_test_orig_docs,\n",
        "                ])\n",
        "            else:\n",
        "                predicted_one_hot = self.model.predict(self.X_test_claims)\n",
        "            predict = np.argmax(predicted_one_hot, axis=-1)\n",
        "\n",
        "            \"\"\"\n",
        "            predicted_one_hot_train = self.model.predict([self.X_train_claims, self.X_train_orig_docs, self.X_train_evid])\n",
        "            predict_train = np.argmax(predicted_one_hot_train, axis=-1)\n",
        "\n",
        "            \n",
        "            # f1 for train data\n",
        "            f1_macro_train = \"\"\n",
        "            if self.print_train_f1 == True:\n",
        "                f1_0_train = f1_score(self.y_train, predict_train, labels=[0], average=None)\n",
        "                f1_1_train = f1_score(self.y_train, predict_train, labels=[1], average=None)\n",
        "                f1_macro_train = (f1_0_train[0] + f1_1_train[0]) / 2\n",
        "                print(\" - train_f1_(macro): \" + str(f1_macro_train))\"\"\"\n",
        "\n",
        "            predicted = [LABELS[int(a)] for a in predict]\n",
        "            actual = [LABELS[int(a)] for a in self.y_test]\n",
        "            # calc FNC score\n",
        "            fold_score, _ = score_submission(actual, predicted)\n",
        "            max_fold_score, _ = score_submission(actual, actual)\n",
        "            fnc_score = fold_score / max_fold_score\n",
        "            print(\" - fnc_score: \" + str(fnc_score))\n",
        "\n",
        "            # f1 for test data\n",
        "            f1_0 = f1_score(self.y_test, predict, labels=[0], average=None)\n",
        "            f1_1 = f1_score(self.y_test, predict, labels=[1], average=None)\n",
        "            f1_2 = f1_score(self.y_test, predict, labels=[2], average=None)\n",
        "            f1_3 = f1_score(self.y_test, predict, labels=[3], average=None)\n",
        "            f1_macro = (f1_0[0] + f1_1[0] + f1_2[0] + f1_3[0]) / 4\n",
        "            print(\" - val_f1_(macro): \" + str(f1_macro))\n",
        "            print(\"\\n\")\n",
        "\n",
        "            header = \"\"\n",
        "            values = \"\"\n",
        "            for key, value in logs.items():\n",
        "                header = header + key + \";\"\n",
        "                values = values + str(value) + \";\"\n",
        "            if epoch == 0:\n",
        "                values = \"\\n\" + header + \"val_f1_macro;\" + \"fnc_score;\" + \"\\n\" + values + str(f1_macro) + str(fnc_score) + \";\"\n",
        "            else:\n",
        "                values += str(f1_macro) + \";\" + str(fnc_score) + \";\"\n",
        "            append_to_loss_monitor_file(values, self.loss_filename)\n",
        "\n",
        "            if epoch >= self.min_epoch-1:  # 9\n",
        "                if f1_macro + self.epsilon <= self.prev_score:\n",
        "                    self.counter += 1\n",
        "                else:\n",
        "                    self.counter = 0\n",
        "                if self.counter >= 2:\n",
        "                    self.model.stop_training = True\n",
        "            #print(\"Counter at \" + str(self.counter))\n",
        "            self.prev_score = f1_macro\n",
        "            #print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubk3uR3FF87-"
      },
      "source": [
        "def append_to_loss_monitor_file(text, filepath):\n",
        "    with open(filepath, 'a+') as the_file:\n",
        "        the_file.write(text+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn-ek0kr-wPk"
      },
      "source": [
        "lr = 0.001\n",
        "optimizer_name = \"rms\"\n",
        "use_class_weights = True\n",
        "epochs = 70\n",
        "min_epoch = 20\n",
        "save_folder = None\n",
        "loss_filename = 'loss.h5'\n",
        "\n",
        "# optimizers\n",
        "if optimizer_name == \"adagrad\":\n",
        "  optimizer = optimizers.Adagrad(lr=lr)\n",
        "  print(\"Used optimizer: adagrad, lr=\"+str(lr))\n",
        "elif optimizer_name == \"adamax\":\n",
        "  optimizer = optimizers.Adamax(lr=lr)\n",
        "  print(\"Used optimizer: adamax, lr=\"+str(lr))\n",
        "elif optimizer_name == \"nadam\":\n",
        "  optimizer = optimizers.Nadam(lr=lr)  # recommended to leave at default params\n",
        "  print(\"Used optimizer: nadam, lr=\"+str(lr))\n",
        "elif optimizer_name == \"rms\":\n",
        "  optimizer = optimizers.RMSprop(lr=lr)  # recommended for RNNs\n",
        "  print(\"Used optimizer: rms, lr=\"+str(lr))\n",
        "elif optimizer_name == \"SGD\":\n",
        "  optimizer = optimizers.SGD(lr=lr)  # recommended for RNNs\n",
        "  print(\"Used optimizer: SGD, lr=\"+str(lr))\n",
        "elif optimizer_name == \"adadelta\":\n",
        "  optimizer = optimizers.Adadelta(lr)  # recommended for RNNs\n",
        "  print(\"Used optimizer: adadelta, lr=\"+str(lr))\n",
        "else:\n",
        "  optimizer = optimizers.Adam(lr=lr)\n",
        "  print(\"Used optimizer: Adam, lr=\" + str(lr))\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer, 'kullback_leibler_divergence', # categorial_crossentropy\n",
        "                           metrics=['accuracy'])\n",
        "if use_class_weights == True:\n",
        "  hist = model.fit([X_train_LSTM, X_train_MLP],\n",
        "                             y_train_one_hot,\n",
        "                             validation_data=([X_test_LSTM, X_test_MLP], y_test_one_hot),\n",
        "                             batch_size=batch_size, epochs=epochs, verbose=1, class_weight=class_weights,\n",
        "                              callbacks=[\n",
        "                               EarlyStoppingOnF1(epochs,\n",
        "                                                 X_test_LSTM, X_test_MLP, y_test,\n",
        "                                                 loss_filename, epsilon=0.0, min_epoch=min_epoch),])\n",
        "else:\n",
        "  hist = model.fit([X_train_LSTM, X_train_MLP],\n",
        "                             y_train_one_hot,\n",
        "                             validation_data=([X_test_LSTM, X_test_MLP], y_test_one_hot),\n",
        "                             batch_size=batch_size, epochs=epochs, verbose=1,\n",
        "                           callbacks=[\n",
        "                               EarlyStoppingOnF1(epochs,\n",
        "                                                 X_test_LSTM, X_test_MLP, y_test,\n",
        "                                                 loss_filename, epsilon=0.0, min_epoch=min_epoch),])\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIzK9drzHIyM"
      },
      "source": [
        "model.save(\"save.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jETm9GZSL25i"
      },
      "source": [
        "model.save('/content/drive/My Drive/persian_stance_deep_data/dataset/models/model_v1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV9kszEgJgH9"
      },
      "source": [
        "print(\"Loading model from:\" + \"save.h5\")\n",
        "model = load_model(\"save.h5\")\n",
        "if (model != None):\n",
        "  X_test_LSTM, X_test_MLP = split_X(X_test, MAX_SEQ_LENGTH)\n",
        "predicted_one_hot = model.predict([X_test_LSTM, X_test_MLP])\n",
        "predicted_int = np.argmax(predicted_one_hot, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rvnwzcrJ_xc"
      },
      "source": [
        "y_test_str = le.inverse_transform(y_test)\n",
        "y_pred_str = le.inverse_transform(predicted_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Lz1i6vJF5-"
      },
      "source": [
        "y_test_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGhFM0zcJLdq"
      },
      "source": [
        "y_pred_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChlIVSHMKeRe"
      },
      "source": [
        "score, cm = score_submission(y_test_str, y_pred_str)\n",
        "fold_score, _ = score_submission(y_test_str, y_pred_str)\n",
        "max_fold_score, _ = score_submission(y_test_str, y_test_str)\n",
        "score = fold_score / max_fold_score\n",
        "\n",
        "print(\"FNC-1 score from restored model: \" +  str(score) +\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlb4p0aCKtIy"
      },
      "source": [
        "def print_confusion_matrix(cm):\n",
        "    lines = []\n",
        "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
        "    line_len = len(header)\n",
        "    lines.append(\"-\"*line_len)\n",
        "    lines.append(header)\n",
        "    lines.append(\"-\"*line_len)\n",
        "\n",
        "    hit = 0\n",
        "    total = 0\n",
        "    for i, row in enumerate(cm):\n",
        "        hit += row[i]\n",
        "        total += sum(row)\n",
        "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n",
        "                                                                   *row))\n",
        "        lines.append(\"-\"*line_len)\n",
        "    print('\\n'.join(lines))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYIs6CZmKyAc"
      },
      "source": [
        "print_confusion_matrix(cm)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}